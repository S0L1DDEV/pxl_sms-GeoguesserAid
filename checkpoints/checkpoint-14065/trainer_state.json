{
  "best_global_step": 8439,
  "best_metric": 1.3499529361724854,
  "best_model_checkpoint": "./checkpoints\\checkpoint-8439",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 14065,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017774617845716316,
      "grad_norm": 45.72555160522461,
      "learning_rate": 8.709562744400995e-07,
      "loss": 5.1679,
      "step": 50
    },
    {
      "epoch": 0.03554923569143263,
      "grad_norm": 37.387176513671875,
      "learning_rate": 1.7596871667259156e-06,
      "loss": 4.3626,
      "step": 100
    },
    {
      "epoch": 0.053323853537148955,
      "grad_norm": 39.64204788208008,
      "learning_rate": 2.6484180590117313e-06,
      "loss": 3.6069,
      "step": 150
    },
    {
      "epoch": 0.07109847138286526,
      "grad_norm": 40.33796310424805,
      "learning_rate": 3.5371489512975476e-06,
      "loss": 3.3746,
      "step": 200
    },
    {
      "epoch": 0.08887308922858159,
      "grad_norm": 44.7415657043457,
      "learning_rate": 4.425879843583363e-06,
      "loss": 3.0482,
      "step": 250
    },
    {
      "epoch": 0.10664770707429791,
      "grad_norm": 66.34918975830078,
      "learning_rate": 5.314610735869179e-06,
      "loss": 2.9366,
      "step": 300
    },
    {
      "epoch": 0.12442232492001422,
      "grad_norm": 57.899330139160156,
      "learning_rate": 6.2033416281549945e-06,
      "loss": 2.7185,
      "step": 350
    },
    {
      "epoch": 0.14219694276573053,
      "grad_norm": 56.05861282348633,
      "learning_rate": 7.092072520440811e-06,
      "loss": 2.6071,
      "step": 400
    },
    {
      "epoch": 0.15997156061144685,
      "grad_norm": 59.65100860595703,
      "learning_rate": 7.980803412726627e-06,
      "loss": 2.4038,
      "step": 450
    },
    {
      "epoch": 0.17774617845716317,
      "grad_norm": 59.519691467285156,
      "learning_rate": 8.869534305012443e-06,
      "loss": 2.4168,
      "step": 500
    },
    {
      "epoch": 0.1955207963028795,
      "grad_norm": 51.024356842041016,
      "learning_rate": 9.758265197298258e-06,
      "loss": 2.3297,
      "step": 550
    },
    {
      "epoch": 0.21329541414859582,
      "grad_norm": 51.21497344970703,
      "learning_rate": 1.0646996089584074e-05,
      "loss": 2.1683,
      "step": 600
    },
    {
      "epoch": 0.23107003199431211,
      "grad_norm": 55.06470489501953,
      "learning_rate": 1.1535726981869891e-05,
      "loss": 2.199,
      "step": 650
    },
    {
      "epoch": 0.24884464984002844,
      "grad_norm": 53.37484359741211,
      "learning_rate": 1.2424457874155706e-05,
      "loss": 2.1537,
      "step": 700
    },
    {
      "epoch": 0.26661926768574473,
      "grad_norm": 43.10935974121094,
      "learning_rate": 1.3313188766441524e-05,
      "loss": 2.0608,
      "step": 750
    },
    {
      "epoch": 0.28439388553146105,
      "grad_norm": 57.46440887451172,
      "learning_rate": 1.4201919658727339e-05,
      "loss": 2.036,
      "step": 800
    },
    {
      "epoch": 0.3021685033771774,
      "grad_norm": 37.48624801635742,
      "learning_rate": 1.5090650551013155e-05,
      "loss": 2.0457,
      "step": 850
    },
    {
      "epoch": 0.3199431212228937,
      "grad_norm": 53.04494094848633,
      "learning_rate": 1.5979381443298968e-05,
      "loss": 1.8508,
      "step": 900
    },
    {
      "epoch": 0.33771773906861,
      "grad_norm": 68.19896697998047,
      "learning_rate": 1.6868112335584785e-05,
      "loss": 1.9957,
      "step": 950
    },
    {
      "epoch": 0.35549235691432635,
      "grad_norm": 70.56356811523438,
      "learning_rate": 1.77568432278706e-05,
      "loss": 1.9539,
      "step": 1000
    },
    {
      "epoch": 0.37326697476004267,
      "grad_norm": 49.84227752685547,
      "learning_rate": 1.8645574120156416e-05,
      "loss": 1.8547,
      "step": 1050
    },
    {
      "epoch": 0.391041592605759,
      "grad_norm": 43.23459243774414,
      "learning_rate": 1.9534305012442234e-05,
      "loss": 1.886,
      "step": 1100
    },
    {
      "epoch": 0.4088162104514753,
      "grad_norm": 42.602691650390625,
      "learning_rate": 2.042303590472805e-05,
      "loss": 2.0184,
      "step": 1150
    },
    {
      "epoch": 0.42659082829719164,
      "grad_norm": 37.47589874267578,
      "learning_rate": 2.1311766797013865e-05,
      "loss": 1.792,
      "step": 1200
    },
    {
      "epoch": 0.4443654461429079,
      "grad_norm": 57.8959846496582,
      "learning_rate": 2.2200497689299682e-05,
      "loss": 2.0696,
      "step": 1250
    },
    {
      "epoch": 0.46214006398862423,
      "grad_norm": 44.53250503540039,
      "learning_rate": 2.30892285815855e-05,
      "loss": 1.7371,
      "step": 1300
    },
    {
      "epoch": 0.47991468183434055,
      "grad_norm": 37.292388916015625,
      "learning_rate": 2.3977959473871313e-05,
      "loss": 1.9644,
      "step": 1350
    },
    {
      "epoch": 0.4976892996800569,
      "grad_norm": 51.556644439697266,
      "learning_rate": 2.4866690366157127e-05,
      "loss": 1.8969,
      "step": 1400
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 46.945472717285156,
      "learning_rate": 2.5755421258442947e-05,
      "loss": 1.9427,
      "step": 1450
    },
    {
      "epoch": 0.5332385353714895,
      "grad_norm": 53.55520248413086,
      "learning_rate": 2.6644152150728764e-05,
      "loss": 1.9012,
      "step": 1500
    },
    {
      "epoch": 0.5510131532172058,
      "grad_norm": 34.33576583862305,
      "learning_rate": 2.7532883043014578e-05,
      "loss": 1.8628,
      "step": 1550
    },
    {
      "epoch": 0.5687877710629221,
      "grad_norm": 35.07330322265625,
      "learning_rate": 2.8421613935300395e-05,
      "loss": 1.9381,
      "step": 1600
    },
    {
      "epoch": 0.5865623889086384,
      "grad_norm": 32.17849349975586,
      "learning_rate": 2.9310344827586206e-05,
      "loss": 1.8474,
      "step": 1650
    },
    {
      "epoch": 0.6043370067543548,
      "grad_norm": 40.79207229614258,
      "learning_rate": 3.0199075719872023e-05,
      "loss": 1.8367,
      "step": 1700
    },
    {
      "epoch": 0.6221116246000711,
      "grad_norm": 47.2532844543457,
      "learning_rate": 3.108780661215784e-05,
      "loss": 1.9656,
      "step": 1750
    },
    {
      "epoch": 0.6398862424457874,
      "grad_norm": 35.42753601074219,
      "learning_rate": 3.197653750444366e-05,
      "loss": 2.0286,
      "step": 1800
    },
    {
      "epoch": 0.6576608602915037,
      "grad_norm": 32.47238540649414,
      "learning_rate": 3.286526839672947e-05,
      "loss": 1.8215,
      "step": 1850
    },
    {
      "epoch": 0.67543547813722,
      "grad_norm": 40.022151947021484,
      "learning_rate": 3.3753999289015285e-05,
      "loss": 1.8714,
      "step": 1900
    },
    {
      "epoch": 0.6932100959829364,
      "grad_norm": 39.149559020996094,
      "learning_rate": 3.46427301813011e-05,
      "loss": 1.9406,
      "step": 1950
    },
    {
      "epoch": 0.7109847138286527,
      "grad_norm": 49.03817367553711,
      "learning_rate": 3.553146107358692e-05,
      "loss": 1.8984,
      "step": 2000
    },
    {
      "epoch": 0.728759331674369,
      "grad_norm": 30.21510124206543,
      "learning_rate": 3.6420191965872736e-05,
      "loss": 1.9127,
      "step": 2050
    },
    {
      "epoch": 0.7465339495200853,
      "grad_norm": 33.946224212646484,
      "learning_rate": 3.7308922858158554e-05,
      "loss": 1.9328,
      "step": 2100
    },
    {
      "epoch": 0.7643085673658017,
      "grad_norm": 38.04575729370117,
      "learning_rate": 3.819765375044437e-05,
      "loss": 1.8885,
      "step": 2150
    },
    {
      "epoch": 0.782083185211518,
      "grad_norm": 42.14911651611328,
      "learning_rate": 3.908638464273018e-05,
      "loss": 1.8715,
      "step": 2200
    },
    {
      "epoch": 0.7998578030572343,
      "grad_norm": 34.796913146972656,
      "learning_rate": 3.9975115535016e-05,
      "loss": 1.9713,
      "step": 2250
    },
    {
      "epoch": 0.8176324209029506,
      "grad_norm": 35.36799240112305,
      "learning_rate": 4.0863846427301816e-05,
      "loss": 1.9537,
      "step": 2300
    },
    {
      "epoch": 0.835407038748667,
      "grad_norm": 31.833845138549805,
      "learning_rate": 4.175257731958763e-05,
      "loss": 1.986,
      "step": 2350
    },
    {
      "epoch": 0.8531816565943833,
      "grad_norm": 31.480195999145508,
      "learning_rate": 4.264130821187345e-05,
      "loss": 1.9089,
      "step": 2400
    },
    {
      "epoch": 0.8709562744400995,
      "grad_norm": 28.059268951416016,
      "learning_rate": 4.353003910415927e-05,
      "loss": 1.9828,
      "step": 2450
    },
    {
      "epoch": 0.8887308922858158,
      "grad_norm": 33.97496032714844,
      "learning_rate": 4.4418769996445084e-05,
      "loss": 1.9426,
      "step": 2500
    },
    {
      "epoch": 0.9065055101315321,
      "grad_norm": 30.06556510925293,
      "learning_rate": 4.5307500888730895e-05,
      "loss": 1.8499,
      "step": 2550
    },
    {
      "epoch": 0.9242801279772485,
      "grad_norm": 21.399444580078125,
      "learning_rate": 4.6196231781016705e-05,
      "loss": 1.8867,
      "step": 2600
    },
    {
      "epoch": 0.9420547458229648,
      "grad_norm": 31.51189613342285,
      "learning_rate": 4.708496267330252e-05,
      "loss": 2.0231,
      "step": 2650
    },
    {
      "epoch": 0.9598293636686811,
      "grad_norm": 35.6729850769043,
      "learning_rate": 4.797369356558834e-05,
      "loss": 1.836,
      "step": 2700
    },
    {
      "epoch": 0.9776039815143974,
      "grad_norm": 28.56304931640625,
      "learning_rate": 4.886242445787416e-05,
      "loss": 1.9399,
      "step": 2750
    },
    {
      "epoch": 0.9953785993601137,
      "grad_norm": 39.50929641723633,
      "learning_rate": 4.9751155350159974e-05,
      "loss": 1.8985,
      "step": 2800
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.8892890214920044,
      "eval_runtime": 579.9912,
      "eval_samples_per_second": 8.621,
      "eval_steps_per_second": 0.54,
      "step": 2813
    },
    {
      "epoch": 1.0131532172058302,
      "grad_norm": 25.659931182861328,
      "learning_rate": 4.992890152861714e-05,
      "loss": 1.7091,
      "step": 2850
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 30.346956253051758,
      "learning_rate": 4.983015365169649e-05,
      "loss": 1.7489,
      "step": 2900
    },
    {
      "epoch": 1.0487024528972626,
      "grad_norm": 34.13132095336914,
      "learning_rate": 4.973140577477585e-05,
      "loss": 1.6874,
      "step": 2950
    },
    {
      "epoch": 1.066477070742979,
      "grad_norm": 26.21145248413086,
      "learning_rate": 4.96326578978552e-05,
      "loss": 1.7069,
      "step": 3000
    },
    {
      "epoch": 1.0842516885886953,
      "grad_norm": 37.5620231628418,
      "learning_rate": 4.953391002093455e-05,
      "loss": 1.6872,
      "step": 3050
    },
    {
      "epoch": 1.1020263064344116,
      "grad_norm": 34.38589096069336,
      "learning_rate": 4.943516214401391e-05,
      "loss": 1.636,
      "step": 3100
    },
    {
      "epoch": 1.119800924280128,
      "grad_norm": 22.090560913085938,
      "learning_rate": 4.933641426709326e-05,
      "loss": 1.7174,
      "step": 3150
    },
    {
      "epoch": 1.1375755421258442,
      "grad_norm": 41.0079231262207,
      "learning_rate": 4.923766639017261e-05,
      "loss": 1.7912,
      "step": 3200
    },
    {
      "epoch": 1.1553501599715605,
      "grad_norm": 22.199108123779297,
      "learning_rate": 4.913891851325197e-05,
      "loss": 1.6326,
      "step": 3250
    },
    {
      "epoch": 1.1731247778172769,
      "grad_norm": 27.446725845336914,
      "learning_rate": 4.904017063633132e-05,
      "loss": 1.5657,
      "step": 3300
    },
    {
      "epoch": 1.1908993956629932,
      "grad_norm": 35.660987854003906,
      "learning_rate": 4.8941422759410674e-05,
      "loss": 1.6159,
      "step": 3350
    },
    {
      "epoch": 1.2086740135087095,
      "grad_norm": 34.567176818847656,
      "learning_rate": 4.884267488249003e-05,
      "loss": 1.5432,
      "step": 3400
    },
    {
      "epoch": 1.2264486313544258,
      "grad_norm": 33.95265579223633,
      "learning_rate": 4.8743927005569384e-05,
      "loss": 1.5938,
      "step": 3450
    },
    {
      "epoch": 1.2442232492001422,
      "grad_norm": 28.62511444091797,
      "learning_rate": 4.8645179128648735e-05,
      "loss": 1.6075,
      "step": 3500
    },
    {
      "epoch": 1.2619978670458585,
      "grad_norm": 24.037513732910156,
      "learning_rate": 4.854643125172809e-05,
      "loss": 1.7417,
      "step": 3550
    },
    {
      "epoch": 1.2797724848915748,
      "grad_norm": 29.832693099975586,
      "learning_rate": 4.8447683374807445e-05,
      "loss": 1.607,
      "step": 3600
    },
    {
      "epoch": 1.2975471027372911,
      "grad_norm": 28.9856014251709,
      "learning_rate": 4.8348935497886796e-05,
      "loss": 1.7013,
      "step": 3650
    },
    {
      "epoch": 1.3153217205830074,
      "grad_norm": 29.250324249267578,
      "learning_rate": 4.8250187620966154e-05,
      "loss": 1.6844,
      "step": 3700
    },
    {
      "epoch": 1.3330963384287238,
      "grad_norm": 28.722253799438477,
      "learning_rate": 4.8151439744045506e-05,
      "loss": 1.6514,
      "step": 3750
    },
    {
      "epoch": 1.35087095627444,
      "grad_norm": 27.43202018737793,
      "learning_rate": 4.8052691867124864e-05,
      "loss": 1.6441,
      "step": 3800
    },
    {
      "epoch": 1.3686455741201564,
      "grad_norm": 30.47803497314453,
      "learning_rate": 4.7953943990204215e-05,
      "loss": 1.6084,
      "step": 3850
    },
    {
      "epoch": 1.3864201919658727,
      "grad_norm": 31.406126022338867,
      "learning_rate": 4.7855196113283567e-05,
      "loss": 1.6217,
      "step": 3900
    },
    {
      "epoch": 1.404194809811589,
      "grad_norm": 26.67877769470215,
      "learning_rate": 4.7756448236362925e-05,
      "loss": 1.6372,
      "step": 3950
    },
    {
      "epoch": 1.4219694276573054,
      "grad_norm": 22.837480545043945,
      "learning_rate": 4.7657700359442276e-05,
      "loss": 1.6203,
      "step": 4000
    },
    {
      "epoch": 1.4397440455030217,
      "grad_norm": 24.148517608642578,
      "learning_rate": 4.755895248252163e-05,
      "loss": 1.4567,
      "step": 4050
    },
    {
      "epoch": 1.457518663348738,
      "grad_norm": 37.102542877197266,
      "learning_rate": 4.7460204605600986e-05,
      "loss": 1.5845,
      "step": 4100
    },
    {
      "epoch": 1.4752932811944544,
      "grad_norm": 31.200496673583984,
      "learning_rate": 4.736145672868034e-05,
      "loss": 1.7188,
      "step": 4150
    },
    {
      "epoch": 1.4930678990401707,
      "grad_norm": 30.396106719970703,
      "learning_rate": 4.726270885175969e-05,
      "loss": 1.5597,
      "step": 4200
    },
    {
      "epoch": 1.510842516885887,
      "grad_norm": 23.89527130126953,
      "learning_rate": 4.716396097483905e-05,
      "loss": 1.5111,
      "step": 4250
    },
    {
      "epoch": 1.5286171347316033,
      "grad_norm": 23.669231414794922,
      "learning_rate": 4.70652130979184e-05,
      "loss": 1.5706,
      "step": 4300
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 42.952919006347656,
      "learning_rate": 4.696646522099775e-05,
      "loss": 1.5084,
      "step": 4350
    },
    {
      "epoch": 1.564166370423036,
      "grad_norm": 29.395689010620117,
      "learning_rate": 4.686771734407711e-05,
      "loss": 1.4579,
      "step": 4400
    },
    {
      "epoch": 1.581940988268752,
      "grad_norm": 20.729915618896484,
      "learning_rate": 4.676896946715646e-05,
      "loss": 1.5591,
      "step": 4450
    },
    {
      "epoch": 1.5997156061144686,
      "grad_norm": 29.782583236694336,
      "learning_rate": 4.667022159023581e-05,
      "loss": 1.543,
      "step": 4500
    },
    {
      "epoch": 1.6174902239601847,
      "grad_norm": 22.65672492980957,
      "learning_rate": 4.657147371331517e-05,
      "loss": 1.4308,
      "step": 4550
    },
    {
      "epoch": 1.6352648418059013,
      "grad_norm": 20.578487396240234,
      "learning_rate": 4.647272583639452e-05,
      "loss": 1.5152,
      "step": 4600
    },
    {
      "epoch": 1.6530394596516174,
      "grad_norm": 21.168766021728516,
      "learning_rate": 4.637397795947387e-05,
      "loss": 1.5689,
      "step": 4650
    },
    {
      "epoch": 1.670814077497334,
      "grad_norm": 27.511762619018555,
      "learning_rate": 4.627523008255323e-05,
      "loss": 1.5315,
      "step": 4700
    },
    {
      "epoch": 1.68858869534305,
      "grad_norm": 26.148204803466797,
      "learning_rate": 4.617648220563258e-05,
      "loss": 1.4508,
      "step": 4750
    },
    {
      "epoch": 1.7063633131887666,
      "grad_norm": 19.7584171295166,
      "learning_rate": 4.607773432871194e-05,
      "loss": 1.6017,
      "step": 4800
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 27.02511215209961,
      "learning_rate": 4.597898645179129e-05,
      "loss": 1.426,
      "step": 4850
    },
    {
      "epoch": 1.7419125488801992,
      "grad_norm": 19.026912689208984,
      "learning_rate": 4.588023857487064e-05,
      "loss": 1.4515,
      "step": 4900
    },
    {
      "epoch": 1.7596871667259153,
      "grad_norm": 19.706106185913086,
      "learning_rate": 4.578149069795e-05,
      "loss": 1.4625,
      "step": 4950
    },
    {
      "epoch": 1.7774617845716318,
      "grad_norm": 24.357330322265625,
      "learning_rate": 4.568274282102935e-05,
      "loss": 1.5009,
      "step": 5000
    },
    {
      "epoch": 1.795236402417348,
      "grad_norm": 19.488529205322266,
      "learning_rate": 4.55839949441087e-05,
      "loss": 1.5199,
      "step": 5050
    },
    {
      "epoch": 1.8130110202630645,
      "grad_norm": 19.253864288330078,
      "learning_rate": 4.548524706718806e-05,
      "loss": 1.5089,
      "step": 5100
    },
    {
      "epoch": 1.8307856381087806,
      "grad_norm": 26.04707145690918,
      "learning_rate": 4.538649919026741e-05,
      "loss": 1.4728,
      "step": 5150
    },
    {
      "epoch": 1.8485602559544971,
      "grad_norm": 31.742950439453125,
      "learning_rate": 4.5287751313346764e-05,
      "loss": 1.5536,
      "step": 5200
    },
    {
      "epoch": 1.8663348738002132,
      "grad_norm": 25.96784210205078,
      "learning_rate": 4.518900343642612e-05,
      "loss": 1.4554,
      "step": 5250
    },
    {
      "epoch": 1.8841094916459296,
      "grad_norm": 22.465164184570312,
      "learning_rate": 4.509025555950547e-05,
      "loss": 1.5746,
      "step": 5300
    },
    {
      "epoch": 1.9018841094916459,
      "grad_norm": 26.8048152923584,
      "learning_rate": 4.4991507682584825e-05,
      "loss": 1.4705,
      "step": 5350
    },
    {
      "epoch": 1.9196587273373622,
      "grad_norm": 27.170921325683594,
      "learning_rate": 4.489275980566418e-05,
      "loss": 1.5308,
      "step": 5400
    },
    {
      "epoch": 1.9374333451830785,
      "grad_norm": 22.280746459960938,
      "learning_rate": 4.4794011928743534e-05,
      "loss": 1.4,
      "step": 5450
    },
    {
      "epoch": 1.9552079630287948,
      "grad_norm": 16.36634635925293,
      "learning_rate": 4.4695264051822886e-05,
      "loss": 1.422,
      "step": 5500
    },
    {
      "epoch": 1.9729825808745112,
      "grad_norm": 18.951019287109375,
      "learning_rate": 4.4596516174902244e-05,
      "loss": 1.3907,
      "step": 5550
    },
    {
      "epoch": 1.9907571987202275,
      "grad_norm": 27.008665084838867,
      "learning_rate": 4.4497768297981595e-05,
      "loss": 1.4914,
      "step": 5600
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.5214927196502686,
      "eval_runtime": 569.0679,
      "eval_samples_per_second": 8.786,
      "eval_steps_per_second": 0.55,
      "step": 5626
    },
    {
      "epoch": 2.008531816565944,
      "grad_norm": 22.139955520629883,
      "learning_rate": 4.4399020421060946e-05,
      "loss": 1.2334,
      "step": 5650
    },
    {
      "epoch": 2.0263064344116604,
      "grad_norm": 19.473690032958984,
      "learning_rate": 4.4300272544140305e-05,
      "loss": 0.9182,
      "step": 5700
    },
    {
      "epoch": 2.0440810522573765,
      "grad_norm": 31.54852294921875,
      "learning_rate": 4.4201524667219656e-05,
      "loss": 1.1257,
      "step": 5750
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 23.8635196685791,
      "learning_rate": 4.410277679029901e-05,
      "loss": 1.174,
      "step": 5800
    },
    {
      "epoch": 2.079630287948809,
      "grad_norm": 18.713970184326172,
      "learning_rate": 4.4004028913378366e-05,
      "loss": 1.0329,
      "step": 5850
    },
    {
      "epoch": 2.097404905794525,
      "grad_norm": 25.963918685913086,
      "learning_rate": 4.390528103645772e-05,
      "loss": 0.9811,
      "step": 5900
    },
    {
      "epoch": 2.1151795236402418,
      "grad_norm": 17.008834838867188,
      "learning_rate": 4.3806533159537075e-05,
      "loss": 1.02,
      "step": 5950
    },
    {
      "epoch": 2.132954141485958,
      "grad_norm": 23.97748565673828,
      "learning_rate": 4.3707785282616426e-05,
      "loss": 1.0977,
      "step": 6000
    },
    {
      "epoch": 2.1507287593316744,
      "grad_norm": 30.05948829650879,
      "learning_rate": 4.360903740569578e-05,
      "loss": 1.0438,
      "step": 6050
    },
    {
      "epoch": 2.1685033771773905,
      "grad_norm": 33.07263946533203,
      "learning_rate": 4.3510289528775136e-05,
      "loss": 1.111,
      "step": 6100
    },
    {
      "epoch": 2.186277995023107,
      "grad_norm": 43.2642936706543,
      "learning_rate": 4.341154165185449e-05,
      "loss": 0.9793,
      "step": 6150
    },
    {
      "epoch": 2.204052612868823,
      "grad_norm": 23.59796905517578,
      "learning_rate": 4.331279377493384e-05,
      "loss": 1.0023,
      "step": 6200
    },
    {
      "epoch": 2.2218272307145397,
      "grad_norm": 19.624591827392578,
      "learning_rate": 4.32140458980132e-05,
      "loss": 1.1256,
      "step": 6250
    },
    {
      "epoch": 2.239601848560256,
      "grad_norm": 32.2612419128418,
      "learning_rate": 4.311529802109255e-05,
      "loss": 1.0984,
      "step": 6300
    },
    {
      "epoch": 2.2573764664059723,
      "grad_norm": 20.0377254486084,
      "learning_rate": 4.30165501441719e-05,
      "loss": 1.0678,
      "step": 6350
    },
    {
      "epoch": 2.2751510842516884,
      "grad_norm": 29.187519073486328,
      "learning_rate": 4.291780226725126e-05,
      "loss": 1.0049,
      "step": 6400
    },
    {
      "epoch": 2.292925702097405,
      "grad_norm": 17.963415145874023,
      "learning_rate": 4.281905439033061e-05,
      "loss": 1.0687,
      "step": 6450
    },
    {
      "epoch": 2.310700319943121,
      "grad_norm": 26.822805404663086,
      "learning_rate": 4.272030651340996e-05,
      "loss": 1.0682,
      "step": 6500
    },
    {
      "epoch": 2.3284749377888376,
      "grad_norm": 27.647830963134766,
      "learning_rate": 4.262155863648932e-05,
      "loss": 1.0849,
      "step": 6550
    },
    {
      "epoch": 2.3462495556345537,
      "grad_norm": 25.731369018554688,
      "learning_rate": 4.252281075956867e-05,
      "loss": 1.0307,
      "step": 6600
    },
    {
      "epoch": 2.3640241734802703,
      "grad_norm": 25.352020263671875,
      "learning_rate": 4.242406288264802e-05,
      "loss": 1.0412,
      "step": 6650
    },
    {
      "epoch": 2.3817987913259864,
      "grad_norm": 21.92180061340332,
      "learning_rate": 4.232531500572738e-05,
      "loss": 1.0782,
      "step": 6700
    },
    {
      "epoch": 2.399573409171703,
      "grad_norm": 12.70843505859375,
      "learning_rate": 4.222656712880673e-05,
      "loss": 1.075,
      "step": 6750
    },
    {
      "epoch": 2.417348027017419,
      "grad_norm": 24.74819564819336,
      "learning_rate": 4.212781925188608e-05,
      "loss": 1.0111,
      "step": 6800
    },
    {
      "epoch": 2.4351226448631356,
      "grad_norm": 25.528308868408203,
      "learning_rate": 4.202907137496544e-05,
      "loss": 1.0418,
      "step": 6850
    },
    {
      "epoch": 2.4528972627088517,
      "grad_norm": 35.784183502197266,
      "learning_rate": 4.193032349804479e-05,
      "loss": 1.1197,
      "step": 6900
    },
    {
      "epoch": 2.470671880554568,
      "grad_norm": 31.773218154907227,
      "learning_rate": 4.183157562112415e-05,
      "loss": 1.0841,
      "step": 6950
    },
    {
      "epoch": 2.4884464984002843,
      "grad_norm": 20.92281723022461,
      "learning_rate": 4.17328277442035e-05,
      "loss": 1.1036,
      "step": 7000
    },
    {
      "epoch": 2.506221116246001,
      "grad_norm": 25.26148223876953,
      "learning_rate": 4.163407986728285e-05,
      "loss": 1.028,
      "step": 7050
    },
    {
      "epoch": 2.523995734091717,
      "grad_norm": 24.617033004760742,
      "learning_rate": 4.153533199036221e-05,
      "loss": 0.9876,
      "step": 7100
    },
    {
      "epoch": 2.5417703519374335,
      "grad_norm": 26.52050018310547,
      "learning_rate": 4.143658411344156e-05,
      "loss": 1.0748,
      "step": 7150
    },
    {
      "epoch": 2.5595449697831496,
      "grad_norm": 26.701807022094727,
      "learning_rate": 4.1337836236520914e-05,
      "loss": 1.0996,
      "step": 7200
    },
    {
      "epoch": 2.5773195876288657,
      "grad_norm": 48.4124755859375,
      "learning_rate": 4.123908835960027e-05,
      "loss": 1.1379,
      "step": 7250
    },
    {
      "epoch": 2.5950942054745822,
      "grad_norm": 28.421138763427734,
      "learning_rate": 4.1140340482679624e-05,
      "loss": 1.0249,
      "step": 7300
    },
    {
      "epoch": 2.612868823320299,
      "grad_norm": 26.9412841796875,
      "learning_rate": 4.1041592605758975e-05,
      "loss": 0.9653,
      "step": 7350
    },
    {
      "epoch": 2.630643441166015,
      "grad_norm": 19.050764083862305,
      "learning_rate": 4.094284472883833e-05,
      "loss": 1.0202,
      "step": 7400
    },
    {
      "epoch": 2.648418059011731,
      "grad_norm": 19.81344223022461,
      "learning_rate": 4.0844096851917684e-05,
      "loss": 1.0404,
      "step": 7450
    },
    {
      "epoch": 2.6661926768574475,
      "grad_norm": 40.7639274597168,
      "learning_rate": 4.0745348974997036e-05,
      "loss": 0.9884,
      "step": 7500
    },
    {
      "epoch": 2.683967294703164,
      "grad_norm": 28.62834358215332,
      "learning_rate": 4.0646601098076394e-05,
      "loss": 1.0541,
      "step": 7550
    },
    {
      "epoch": 2.70174191254888,
      "grad_norm": 29.86837387084961,
      "learning_rate": 4.0547853221155745e-05,
      "loss": 1.0564,
      "step": 7600
    },
    {
      "epoch": 2.7195165303945963,
      "grad_norm": 29.453645706176758,
      "learning_rate": 4.04491053442351e-05,
      "loss": 1.0581,
      "step": 7650
    },
    {
      "epoch": 2.737291148240313,
      "grad_norm": 25.032562255859375,
      "learning_rate": 4.0350357467314455e-05,
      "loss": 0.9444,
      "step": 7700
    },
    {
      "epoch": 2.7550657660860294,
      "grad_norm": 24.643957138061523,
      "learning_rate": 4.0251609590393806e-05,
      "loss": 1.1764,
      "step": 7750
    },
    {
      "epoch": 2.7728403839317455,
      "grad_norm": 21.62310028076172,
      "learning_rate": 4.015286171347316e-05,
      "loss": 1.0026,
      "step": 7800
    },
    {
      "epoch": 2.7906150017774616,
      "grad_norm": 32.50385665893555,
      "learning_rate": 4.0054113836552516e-05,
      "loss": 1.0692,
      "step": 7850
    },
    {
      "epoch": 2.808389619623178,
      "grad_norm": 29.179288864135742,
      "learning_rate": 3.995536595963187e-05,
      "loss": 1.1436,
      "step": 7900
    },
    {
      "epoch": 2.8261642374688947,
      "grad_norm": 17.948240280151367,
      "learning_rate": 3.9856618082711225e-05,
      "loss": 1.0807,
      "step": 7950
    },
    {
      "epoch": 2.8439388553146108,
      "grad_norm": 30.448869705200195,
      "learning_rate": 3.975787020579058e-05,
      "loss": 1.0507,
      "step": 8000
    },
    {
      "epoch": 2.861713473160327,
      "grad_norm": 23.68577003479004,
      "learning_rate": 3.965912232886993e-05,
      "loss": 1.081,
      "step": 8050
    },
    {
      "epoch": 2.8794880910060434,
      "grad_norm": 33.38752365112305,
      "learning_rate": 3.9560374451949286e-05,
      "loss": 0.9865,
      "step": 8100
    },
    {
      "epoch": 2.89726270885176,
      "grad_norm": 21.95579719543457,
      "learning_rate": 3.946162657502864e-05,
      "loss": 1.0675,
      "step": 8150
    },
    {
      "epoch": 2.915037326697476,
      "grad_norm": 32.327880859375,
      "learning_rate": 3.936287869810799e-05,
      "loss": 1.0284,
      "step": 8200
    },
    {
      "epoch": 2.932811944543192,
      "grad_norm": 17.930652618408203,
      "learning_rate": 3.926413082118735e-05,
      "loss": 1.0355,
      "step": 8250
    },
    {
      "epoch": 2.9505865623889087,
      "grad_norm": 34.20661163330078,
      "learning_rate": 3.91653829442667e-05,
      "loss": 1.0186,
      "step": 8300
    },
    {
      "epoch": 2.968361180234625,
      "grad_norm": 27.941930770874023,
      "learning_rate": 3.906663506734605e-05,
      "loss": 1.0054,
      "step": 8350
    },
    {
      "epoch": 2.9861357980803414,
      "grad_norm": 30.37055015563965,
      "learning_rate": 3.896788719042541e-05,
      "loss": 0.9835,
      "step": 8400
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.3499529361724854,
      "eval_runtime": 595.2934,
      "eval_samples_per_second": 8.399,
      "eval_steps_per_second": 0.526,
      "step": 8439
    },
    {
      "epoch": 3.0039104159260575,
      "grad_norm": 20.970138549804688,
      "learning_rate": 3.886913931350476e-05,
      "loss": 0.8875,
      "step": 8450
    },
    {
      "epoch": 3.021685033771774,
      "grad_norm": 14.04200553894043,
      "learning_rate": 3.877039143658411e-05,
      "loss": 0.6099,
      "step": 8500
    },
    {
      "epoch": 3.03945965161749,
      "grad_norm": 18.750411987304688,
      "learning_rate": 3.867164355966347e-05,
      "loss": 0.6424,
      "step": 8550
    },
    {
      "epoch": 3.0572342694632066,
      "grad_norm": 21.11343765258789,
      "learning_rate": 3.857289568274282e-05,
      "loss": 0.5792,
      "step": 8600
    },
    {
      "epoch": 3.0750088873089227,
      "grad_norm": 21.136653900146484,
      "learning_rate": 3.847414780582217e-05,
      "loss": 0.5505,
      "step": 8650
    },
    {
      "epoch": 3.0927835051546393,
      "grad_norm": 25.195865631103516,
      "learning_rate": 3.837539992890153e-05,
      "loss": 0.6108,
      "step": 8700
    },
    {
      "epoch": 3.1105581230003554,
      "grad_norm": 24.272708892822266,
      "learning_rate": 3.827665205198088e-05,
      "loss": 0.6472,
      "step": 8750
    },
    {
      "epoch": 3.128332740846072,
      "grad_norm": 11.214747428894043,
      "learning_rate": 3.817790417506023e-05,
      "loss": 0.5809,
      "step": 8800
    },
    {
      "epoch": 3.146107358691788,
      "grad_norm": 18.944015502929688,
      "learning_rate": 3.807915629813959e-05,
      "loss": 0.5654,
      "step": 8850
    },
    {
      "epoch": 3.1638819765375046,
      "grad_norm": 26.34320640563965,
      "learning_rate": 3.798040842121894e-05,
      "loss": 0.6507,
      "step": 8900
    },
    {
      "epoch": 3.1816565943832207,
      "grad_norm": 27.523849487304688,
      "learning_rate": 3.78816605442983e-05,
      "loss": 0.5862,
      "step": 8950
    },
    {
      "epoch": 3.1994312122289372,
      "grad_norm": 19.137331008911133,
      "learning_rate": 3.778291266737765e-05,
      "loss": 0.6941,
      "step": 9000
    },
    {
      "epoch": 3.2172058300746533,
      "grad_norm": 38.839847564697266,
      "learning_rate": 3.7684164790457003e-05,
      "loss": 0.588,
      "step": 9050
    },
    {
      "epoch": 3.23498044792037,
      "grad_norm": 24.464548110961914,
      "learning_rate": 3.758541691353636e-05,
      "loss": 0.761,
      "step": 9100
    },
    {
      "epoch": 3.252755065766086,
      "grad_norm": 31.030643463134766,
      "learning_rate": 3.748666903661571e-05,
      "loss": 0.5936,
      "step": 9150
    },
    {
      "epoch": 3.2705296836118025,
      "grad_norm": 13.451333999633789,
      "learning_rate": 3.7387921159695064e-05,
      "loss": 0.7235,
      "step": 9200
    },
    {
      "epoch": 3.2883043014575186,
      "grad_norm": 26.44865608215332,
      "learning_rate": 3.728917328277442e-05,
      "loss": 0.5973,
      "step": 9250
    },
    {
      "epoch": 3.306078919303235,
      "grad_norm": 17.371505737304688,
      "learning_rate": 3.7190425405853774e-05,
      "loss": 0.6346,
      "step": 9300
    },
    {
      "epoch": 3.3238535371489513,
      "grad_norm": 35.123538970947266,
      "learning_rate": 3.7091677528933125e-05,
      "loss": 0.5805,
      "step": 9350
    },
    {
      "epoch": 3.341628154994668,
      "grad_norm": 42.86513900756836,
      "learning_rate": 3.6992929652012483e-05,
      "loss": 0.6318,
      "step": 9400
    },
    {
      "epoch": 3.359402772840384,
      "grad_norm": 23.84062957763672,
      "learning_rate": 3.6894181775091835e-05,
      "loss": 0.5973,
      "step": 9450
    },
    {
      "epoch": 3.3771773906861,
      "grad_norm": 29.349319458007812,
      "learning_rate": 3.6795433898171186e-05,
      "loss": 0.6459,
      "step": 9500
    },
    {
      "epoch": 3.3949520085318166,
      "grad_norm": 19.391407012939453,
      "learning_rate": 3.6696686021250544e-05,
      "loss": 0.632,
      "step": 9550
    },
    {
      "epoch": 3.412726626377533,
      "grad_norm": 24.438249588012695,
      "learning_rate": 3.6597938144329896e-05,
      "loss": 0.7181,
      "step": 9600
    },
    {
      "epoch": 3.430501244223249,
      "grad_norm": 14.284900665283203,
      "learning_rate": 3.649919026740925e-05,
      "loss": 0.6143,
      "step": 9650
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 21.91012191772461,
      "learning_rate": 3.6400442390488605e-05,
      "loss": 0.6533,
      "step": 9700
    },
    {
      "epoch": 3.466050479914682,
      "grad_norm": 26.20189094543457,
      "learning_rate": 3.630169451356796e-05,
      "loss": 0.6168,
      "step": 9750
    },
    {
      "epoch": 3.4838250977603984,
      "grad_norm": 28.79902458190918,
      "learning_rate": 3.620294663664731e-05,
      "loss": 0.6459,
      "step": 9800
    },
    {
      "epoch": 3.5015997156061145,
      "grad_norm": 23.52932357788086,
      "learning_rate": 3.6104198759726666e-05,
      "loss": 0.6944,
      "step": 9850
    },
    {
      "epoch": 3.5193743334518306,
      "grad_norm": 15.69887924194336,
      "learning_rate": 3.600545088280602e-05,
      "loss": 0.6383,
      "step": 9900
    },
    {
      "epoch": 3.537148951297547,
      "grad_norm": 27.866592407226562,
      "learning_rate": 3.5906703005885376e-05,
      "loss": 0.5906,
      "step": 9950
    },
    {
      "epoch": 3.5549235691432637,
      "grad_norm": 22.662351608276367,
      "learning_rate": 3.580795512896473e-05,
      "loss": 0.6757,
      "step": 10000
    },
    {
      "epoch": 3.57269818698898,
      "grad_norm": 23.2382869720459,
      "learning_rate": 3.570920725204408e-05,
      "loss": 0.6874,
      "step": 10050
    },
    {
      "epoch": 3.590472804834696,
      "grad_norm": 32.9306526184082,
      "learning_rate": 3.561045937512344e-05,
      "loss": 0.602,
      "step": 10100
    },
    {
      "epoch": 3.6082474226804124,
      "grad_norm": 20.117294311523438,
      "learning_rate": 3.551171149820279e-05,
      "loss": 0.645,
      "step": 10150
    },
    {
      "epoch": 3.6260220405261285,
      "grad_norm": 19.04990577697754,
      "learning_rate": 3.541296362128214e-05,
      "loss": 0.6918,
      "step": 10200
    },
    {
      "epoch": 3.643796658371845,
      "grad_norm": 30.83713150024414,
      "learning_rate": 3.53142157443615e-05,
      "loss": 0.6288,
      "step": 10250
    },
    {
      "epoch": 3.661571276217561,
      "grad_norm": 29.392410278320312,
      "learning_rate": 3.5215467867440856e-05,
      "loss": 0.546,
      "step": 10300
    },
    {
      "epoch": 3.6793458940632777,
      "grad_norm": 20.53127670288086,
      "learning_rate": 3.511671999052021e-05,
      "loss": 0.6232,
      "step": 10350
    },
    {
      "epoch": 3.697120511908994,
      "grad_norm": 28.859729766845703,
      "learning_rate": 3.501797211359956e-05,
      "loss": 0.6437,
      "step": 10400
    },
    {
      "epoch": 3.7148951297547104,
      "grad_norm": 26.614883422851562,
      "learning_rate": 3.491922423667892e-05,
      "loss": 0.649,
      "step": 10450
    },
    {
      "epoch": 3.7326697476004265,
      "grad_norm": 15.354384422302246,
      "learning_rate": 3.482047635975827e-05,
      "loss": 0.5608,
      "step": 10500
    },
    {
      "epoch": 3.750444365446143,
      "grad_norm": 34.40671157836914,
      "learning_rate": 3.4721728482837626e-05,
      "loss": 0.6338,
      "step": 10550
    },
    {
      "epoch": 3.768218983291859,
      "grad_norm": 33.3489990234375,
      "learning_rate": 3.462298060591698e-05,
      "loss": 0.6203,
      "step": 10600
    },
    {
      "epoch": 3.7859936011375757,
      "grad_norm": 30.411245346069336,
      "learning_rate": 3.452423272899633e-05,
      "loss": 0.6189,
      "step": 10650
    },
    {
      "epoch": 3.8037682189832918,
      "grad_norm": 18.49363899230957,
      "learning_rate": 3.442548485207569e-05,
      "loss": 0.653,
      "step": 10700
    },
    {
      "epoch": 3.8215428368290083,
      "grad_norm": 18.03886604309082,
      "learning_rate": 3.432673697515504e-05,
      "loss": 0.6549,
      "step": 10750
    },
    {
      "epoch": 3.8393174546747244,
      "grad_norm": 32.8499641418457,
      "learning_rate": 3.422798909823439e-05,
      "loss": 0.6347,
      "step": 10800
    },
    {
      "epoch": 3.857092072520441,
      "grad_norm": 21.178979873657227,
      "learning_rate": 3.412924122131375e-05,
      "loss": 0.6284,
      "step": 10850
    },
    {
      "epoch": 3.874866690366157,
      "grad_norm": 19.413715362548828,
      "learning_rate": 3.40304933443931e-05,
      "loss": 0.6348,
      "step": 10900
    },
    {
      "epoch": 3.8926413082118736,
      "grad_norm": 25.282211303710938,
      "learning_rate": 3.393174546747245e-05,
      "loss": 0.5854,
      "step": 10950
    },
    {
      "epoch": 3.9104159260575897,
      "grad_norm": 17.415884017944336,
      "learning_rate": 3.383299759055181e-05,
      "loss": 0.6568,
      "step": 11000
    },
    {
      "epoch": 3.9281905439033062,
      "grad_norm": 18.71302604675293,
      "learning_rate": 3.373424971363116e-05,
      "loss": 0.7064,
      "step": 11050
    },
    {
      "epoch": 3.9459651617490223,
      "grad_norm": 23.446887969970703,
      "learning_rate": 3.363550183671051e-05,
      "loss": 0.5959,
      "step": 11100
    },
    {
      "epoch": 3.9637397795947384,
      "grad_norm": 35.751251220703125,
      "learning_rate": 3.353675395978987e-05,
      "loss": 0.6836,
      "step": 11150
    },
    {
      "epoch": 3.981514397440455,
      "grad_norm": 21.46914291381836,
      "learning_rate": 3.343800608286922e-05,
      "loss": 0.6329,
      "step": 11200
    },
    {
      "epoch": 3.9992890152861715,
      "grad_norm": 20.236717224121094,
      "learning_rate": 3.333925820594857e-05,
      "loss": 0.6809,
      "step": 11250
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.4252818822860718,
      "eval_runtime": 612.7145,
      "eval_samples_per_second": 8.16,
      "eval_steps_per_second": 0.511,
      "step": 11252
    },
    {
      "epoch": 4.017063633131888,
      "grad_norm": 26.28738784790039,
      "learning_rate": 3.324051032902793e-05,
      "loss": 0.3489,
      "step": 11300
    },
    {
      "epoch": 4.034838250977604,
      "grad_norm": 5.1397809982299805,
      "learning_rate": 3.314176245210728e-05,
      "loss": 0.3019,
      "step": 11350
    },
    {
      "epoch": 4.052612868823321,
      "grad_norm": 16.36956214904785,
      "learning_rate": 3.3043014575186634e-05,
      "loss": 0.2902,
      "step": 11400
    },
    {
      "epoch": 4.070387486669037,
      "grad_norm": 20.106477737426758,
      "learning_rate": 3.294426669826599e-05,
      "loss": 0.3157,
      "step": 11450
    },
    {
      "epoch": 4.088162104514753,
      "grad_norm": 44.09709930419922,
      "learning_rate": 3.2845518821345343e-05,
      "loss": 0.3435,
      "step": 11500
    },
    {
      "epoch": 4.105936722360469,
      "grad_norm": 20.194002151489258,
      "learning_rate": 3.27467709444247e-05,
      "loss": 0.3236,
      "step": 11550
    },
    {
      "epoch": 4.123711340206185,
      "grad_norm": 30.338281631469727,
      "learning_rate": 3.264802306750405e-05,
      "loss": 0.3514,
      "step": 11600
    },
    {
      "epoch": 4.141485958051902,
      "grad_norm": 22.332063674926758,
      "learning_rate": 3.2549275190583404e-05,
      "loss": 0.3573,
      "step": 11650
    },
    {
      "epoch": 4.159260575897618,
      "grad_norm": 28.593414306640625,
      "learning_rate": 3.245052731366276e-05,
      "loss": 0.3544,
      "step": 11700
    },
    {
      "epoch": 4.177035193743334,
      "grad_norm": 33.09299850463867,
      "learning_rate": 3.2351779436742114e-05,
      "loss": 0.289,
      "step": 11750
    },
    {
      "epoch": 4.19480981158905,
      "grad_norm": 7.052907943725586,
      "learning_rate": 3.2253031559821465e-05,
      "loss": 0.3145,
      "step": 11800
    },
    {
      "epoch": 4.212584429434767,
      "grad_norm": 15.328741073608398,
      "learning_rate": 3.2154283682900823e-05,
      "loss": 0.3659,
      "step": 11850
    },
    {
      "epoch": 4.2303590472804835,
      "grad_norm": 35.098175048828125,
      "learning_rate": 3.2055535805980175e-05,
      "loss": 0.2856,
      "step": 11900
    },
    {
      "epoch": 4.2481336651262,
      "grad_norm": 24.490201950073242,
      "learning_rate": 3.1956787929059526e-05,
      "loss": 0.2929,
      "step": 11950
    },
    {
      "epoch": 4.265908282971916,
      "grad_norm": 14.10155963897705,
      "learning_rate": 3.1858040052138884e-05,
      "loss": 0.3125,
      "step": 12000
    },
    {
      "epoch": 4.283682900817633,
      "grad_norm": 28.332578659057617,
      "learning_rate": 3.1759292175218236e-05,
      "loss": 0.3217,
      "step": 12050
    },
    {
      "epoch": 4.301457518663349,
      "grad_norm": 19.941715240478516,
      "learning_rate": 3.166054429829759e-05,
      "loss": 0.3503,
      "step": 12100
    },
    {
      "epoch": 4.319232136509065,
      "grad_norm": 31.504701614379883,
      "learning_rate": 3.1561796421376945e-05,
      "loss": 0.3461,
      "step": 12150
    },
    {
      "epoch": 4.337006754354781,
      "grad_norm": 9.094016075134277,
      "learning_rate": 3.14630485444563e-05,
      "loss": 0.3116,
      "step": 12200
    },
    {
      "epoch": 4.354781372200498,
      "grad_norm": 28.628734588623047,
      "learning_rate": 3.136430066753565e-05,
      "loss": 0.2764,
      "step": 12250
    },
    {
      "epoch": 4.372555990046214,
      "grad_norm": 23.97233772277832,
      "learning_rate": 3.1265552790615006e-05,
      "loss": 0.3863,
      "step": 12300
    },
    {
      "epoch": 4.39033060789193,
      "grad_norm": 13.69547176361084,
      "learning_rate": 3.116680491369436e-05,
      "loss": 0.3379,
      "step": 12350
    },
    {
      "epoch": 4.408105225737646,
      "grad_norm": 22.47005271911621,
      "learning_rate": 3.106805703677371e-05,
      "loss": 0.2832,
      "step": 12400
    },
    {
      "epoch": 4.425879843583363,
      "grad_norm": 29.116649627685547,
      "learning_rate": 3.096930915985307e-05,
      "loss": 0.3273,
      "step": 12450
    },
    {
      "epoch": 4.443654461429079,
      "grad_norm": 27.6177978515625,
      "learning_rate": 3.087056128293242e-05,
      "loss": 0.378,
      "step": 12500
    },
    {
      "epoch": 4.4614290792747955,
      "grad_norm": 33.54322052001953,
      "learning_rate": 3.077181340601178e-05,
      "loss": 0.3566,
      "step": 12550
    },
    {
      "epoch": 4.479203697120512,
      "grad_norm": 13.55495548248291,
      "learning_rate": 3.067306552909113e-05,
      "loss": 0.38,
      "step": 12600
    },
    {
      "epoch": 4.496978314966229,
      "grad_norm": 10.155524253845215,
      "learning_rate": 3.057431765217048e-05,
      "loss": 0.3664,
      "step": 12650
    },
    {
      "epoch": 4.514752932811945,
      "grad_norm": 34.01736831665039,
      "learning_rate": 3.0475569775249834e-05,
      "loss": 0.3195,
      "step": 12700
    },
    {
      "epoch": 4.532527550657661,
      "grad_norm": 6.52284574508667,
      "learning_rate": 3.037682189832919e-05,
      "loss": 0.3033,
      "step": 12750
    },
    {
      "epoch": 4.550302168503377,
      "grad_norm": 25.09425926208496,
      "learning_rate": 3.0278074021408544e-05,
      "loss": 0.4003,
      "step": 12800
    },
    {
      "epoch": 4.568076786349094,
      "grad_norm": 20.682859420776367,
      "learning_rate": 3.0179326144487895e-05,
      "loss": 0.3694,
      "step": 12850
    },
    {
      "epoch": 4.58585140419481,
      "grad_norm": 9.262455940246582,
      "learning_rate": 3.008057826756725e-05,
      "loss": 0.3388,
      "step": 12900
    },
    {
      "epoch": 4.603626022040526,
      "grad_norm": 18.448575973510742,
      "learning_rate": 2.9981830390646605e-05,
      "loss": 0.3431,
      "step": 12950
    },
    {
      "epoch": 4.621400639886242,
      "grad_norm": 29.990999221801758,
      "learning_rate": 2.9883082513725956e-05,
      "loss": 0.3937,
      "step": 13000
    },
    {
      "epoch": 4.639175257731958,
      "grad_norm": 36.05508804321289,
      "learning_rate": 2.978433463680531e-05,
      "loss": 0.3453,
      "step": 13050
    },
    {
      "epoch": 4.656949875577675,
      "grad_norm": 16.277252197265625,
      "learning_rate": 2.9685586759884666e-05,
      "loss": 0.3493,
      "step": 13100
    },
    {
      "epoch": 4.674724493423391,
      "grad_norm": 20.95882797241211,
      "learning_rate": 2.9586838882964017e-05,
      "loss": 0.3393,
      "step": 13150
    },
    {
      "epoch": 4.6924991112691075,
      "grad_norm": 10.493337631225586,
      "learning_rate": 2.9488091006043372e-05,
      "loss": 0.3629,
      "step": 13200
    },
    {
      "epoch": 4.7102737291148244,
      "grad_norm": 18.928951263427734,
      "learning_rate": 2.9389343129122727e-05,
      "loss": 0.3347,
      "step": 13250
    },
    {
      "epoch": 4.7280483469605405,
      "grad_norm": 8.087231636047363,
      "learning_rate": 2.929059525220208e-05,
      "loss": 0.3495,
      "step": 13300
    },
    {
      "epoch": 4.745822964806257,
      "grad_norm": 21.5911865234375,
      "learning_rate": 2.9191847375281433e-05,
      "loss": 0.3206,
      "step": 13350
    },
    {
      "epoch": 4.763597582651973,
      "grad_norm": 18.53110694885254,
      "learning_rate": 2.9093099498360788e-05,
      "loss": 0.3464,
      "step": 13400
    },
    {
      "epoch": 4.781372200497689,
      "grad_norm": 43.2369384765625,
      "learning_rate": 2.8994351621440142e-05,
      "loss": 0.371,
      "step": 13450
    },
    {
      "epoch": 4.799146818343406,
      "grad_norm": 20.332012176513672,
      "learning_rate": 2.8895603744519494e-05,
      "loss": 0.3741,
      "step": 13500
    },
    {
      "epoch": 4.816921436189122,
      "grad_norm": 25.648473739624023,
      "learning_rate": 2.879685586759885e-05,
      "loss": 0.3153,
      "step": 13550
    },
    {
      "epoch": 4.834696054034838,
      "grad_norm": 27.965686798095703,
      "learning_rate": 2.8698107990678203e-05,
      "loss": 0.372,
      "step": 13600
    },
    {
      "epoch": 4.852470671880555,
      "grad_norm": 25.140275955200195,
      "learning_rate": 2.8599360113757555e-05,
      "loss": 0.3491,
      "step": 13650
    },
    {
      "epoch": 4.870245289726271,
      "grad_norm": 16.192298889160156,
      "learning_rate": 2.850061223683691e-05,
      "loss": 0.3339,
      "step": 13700
    },
    {
      "epoch": 4.888019907571987,
      "grad_norm": 14.041748046875,
      "learning_rate": 2.8401864359916264e-05,
      "loss": 0.2957,
      "step": 13750
    },
    {
      "epoch": 4.905794525417703,
      "grad_norm": 18.597122192382812,
      "learning_rate": 2.830311648299562e-05,
      "loss": 0.3395,
      "step": 13800
    },
    {
      "epoch": 4.923569143263419,
      "grad_norm": 18.48052215576172,
      "learning_rate": 2.820436860607497e-05,
      "loss": 0.3563,
      "step": 13850
    },
    {
      "epoch": 4.941343761109136,
      "grad_norm": 32.82701110839844,
      "learning_rate": 2.8105620729154325e-05,
      "loss": 0.3499,
      "step": 13900
    },
    {
      "epoch": 4.9591183789548525,
      "grad_norm": 9.885085105895996,
      "learning_rate": 2.800687285223368e-05,
      "loss": 0.3601,
      "step": 13950
    },
    {
      "epoch": 4.976892996800569,
      "grad_norm": 20.4886474609375,
      "learning_rate": 2.790812497531303e-05,
      "loss": 0.352,
      "step": 14000
    },
    {
      "epoch": 4.994667614646285,
      "grad_norm": 23.855506896972656,
      "learning_rate": 2.7809377098392386e-05,
      "loss": 0.3583,
      "step": 14050
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.471807837486267,
      "eval_runtime": 613.0327,
      "eval_samples_per_second": 8.156,
      "eval_steps_per_second": 0.511,
      "step": 14065
    }
  ],
  "logging_steps": 50,
  "max_steps": 28130,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.886279397482705e+19,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
