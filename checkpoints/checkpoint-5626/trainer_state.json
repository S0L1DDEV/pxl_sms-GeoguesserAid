{
  "best_global_step": 5626,
  "best_metric": 1.5214927196502686,
  "best_model_checkpoint": "./checkpoints\\checkpoint-5626",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 5626,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017774617845716316,
      "grad_norm": 45.72555160522461,
      "learning_rate": 8.709562744400995e-07,
      "loss": 5.1679,
      "step": 50
    },
    {
      "epoch": 0.03554923569143263,
      "grad_norm": 37.387176513671875,
      "learning_rate": 1.7596871667259156e-06,
      "loss": 4.3626,
      "step": 100
    },
    {
      "epoch": 0.053323853537148955,
      "grad_norm": 39.64204788208008,
      "learning_rate": 2.6484180590117313e-06,
      "loss": 3.6069,
      "step": 150
    },
    {
      "epoch": 0.07109847138286526,
      "grad_norm": 40.33796310424805,
      "learning_rate": 3.5371489512975476e-06,
      "loss": 3.3746,
      "step": 200
    },
    {
      "epoch": 0.08887308922858159,
      "grad_norm": 44.7415657043457,
      "learning_rate": 4.425879843583363e-06,
      "loss": 3.0482,
      "step": 250
    },
    {
      "epoch": 0.10664770707429791,
      "grad_norm": 66.34918975830078,
      "learning_rate": 5.314610735869179e-06,
      "loss": 2.9366,
      "step": 300
    },
    {
      "epoch": 0.12442232492001422,
      "grad_norm": 57.899330139160156,
      "learning_rate": 6.2033416281549945e-06,
      "loss": 2.7185,
      "step": 350
    },
    {
      "epoch": 0.14219694276573053,
      "grad_norm": 56.05861282348633,
      "learning_rate": 7.092072520440811e-06,
      "loss": 2.6071,
      "step": 400
    },
    {
      "epoch": 0.15997156061144685,
      "grad_norm": 59.65100860595703,
      "learning_rate": 7.980803412726627e-06,
      "loss": 2.4038,
      "step": 450
    },
    {
      "epoch": 0.17774617845716317,
      "grad_norm": 59.519691467285156,
      "learning_rate": 8.869534305012443e-06,
      "loss": 2.4168,
      "step": 500
    },
    {
      "epoch": 0.1955207963028795,
      "grad_norm": 51.024356842041016,
      "learning_rate": 9.758265197298258e-06,
      "loss": 2.3297,
      "step": 550
    },
    {
      "epoch": 0.21329541414859582,
      "grad_norm": 51.21497344970703,
      "learning_rate": 1.0646996089584074e-05,
      "loss": 2.1683,
      "step": 600
    },
    {
      "epoch": 0.23107003199431211,
      "grad_norm": 55.06470489501953,
      "learning_rate": 1.1535726981869891e-05,
      "loss": 2.199,
      "step": 650
    },
    {
      "epoch": 0.24884464984002844,
      "grad_norm": 53.37484359741211,
      "learning_rate": 1.2424457874155706e-05,
      "loss": 2.1537,
      "step": 700
    },
    {
      "epoch": 0.26661926768574473,
      "grad_norm": 43.10935974121094,
      "learning_rate": 1.3313188766441524e-05,
      "loss": 2.0608,
      "step": 750
    },
    {
      "epoch": 0.28439388553146105,
      "grad_norm": 57.46440887451172,
      "learning_rate": 1.4201919658727339e-05,
      "loss": 2.036,
      "step": 800
    },
    {
      "epoch": 0.3021685033771774,
      "grad_norm": 37.48624801635742,
      "learning_rate": 1.5090650551013155e-05,
      "loss": 2.0457,
      "step": 850
    },
    {
      "epoch": 0.3199431212228937,
      "grad_norm": 53.04494094848633,
      "learning_rate": 1.5979381443298968e-05,
      "loss": 1.8508,
      "step": 900
    },
    {
      "epoch": 0.33771773906861,
      "grad_norm": 68.19896697998047,
      "learning_rate": 1.6868112335584785e-05,
      "loss": 1.9957,
      "step": 950
    },
    {
      "epoch": 0.35549235691432635,
      "grad_norm": 70.56356811523438,
      "learning_rate": 1.77568432278706e-05,
      "loss": 1.9539,
      "step": 1000
    },
    {
      "epoch": 0.37326697476004267,
      "grad_norm": 49.84227752685547,
      "learning_rate": 1.8645574120156416e-05,
      "loss": 1.8547,
      "step": 1050
    },
    {
      "epoch": 0.391041592605759,
      "grad_norm": 43.23459243774414,
      "learning_rate": 1.9534305012442234e-05,
      "loss": 1.886,
      "step": 1100
    },
    {
      "epoch": 0.4088162104514753,
      "grad_norm": 42.602691650390625,
      "learning_rate": 2.042303590472805e-05,
      "loss": 2.0184,
      "step": 1150
    },
    {
      "epoch": 0.42659082829719164,
      "grad_norm": 37.47589874267578,
      "learning_rate": 2.1311766797013865e-05,
      "loss": 1.792,
      "step": 1200
    },
    {
      "epoch": 0.4443654461429079,
      "grad_norm": 57.8959846496582,
      "learning_rate": 2.2200497689299682e-05,
      "loss": 2.0696,
      "step": 1250
    },
    {
      "epoch": 0.46214006398862423,
      "grad_norm": 44.53250503540039,
      "learning_rate": 2.30892285815855e-05,
      "loss": 1.7371,
      "step": 1300
    },
    {
      "epoch": 0.47991468183434055,
      "grad_norm": 37.292388916015625,
      "learning_rate": 2.3977959473871313e-05,
      "loss": 1.9644,
      "step": 1350
    },
    {
      "epoch": 0.4976892996800569,
      "grad_norm": 51.556644439697266,
      "learning_rate": 2.4866690366157127e-05,
      "loss": 1.8969,
      "step": 1400
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 46.945472717285156,
      "learning_rate": 2.5755421258442947e-05,
      "loss": 1.9427,
      "step": 1450
    },
    {
      "epoch": 0.5332385353714895,
      "grad_norm": 53.55520248413086,
      "learning_rate": 2.6644152150728764e-05,
      "loss": 1.9012,
      "step": 1500
    },
    {
      "epoch": 0.5510131532172058,
      "grad_norm": 34.33576583862305,
      "learning_rate": 2.7532883043014578e-05,
      "loss": 1.8628,
      "step": 1550
    },
    {
      "epoch": 0.5687877710629221,
      "grad_norm": 35.07330322265625,
      "learning_rate": 2.8421613935300395e-05,
      "loss": 1.9381,
      "step": 1600
    },
    {
      "epoch": 0.5865623889086384,
      "grad_norm": 32.17849349975586,
      "learning_rate": 2.9310344827586206e-05,
      "loss": 1.8474,
      "step": 1650
    },
    {
      "epoch": 0.6043370067543548,
      "grad_norm": 40.79207229614258,
      "learning_rate": 3.0199075719872023e-05,
      "loss": 1.8367,
      "step": 1700
    },
    {
      "epoch": 0.6221116246000711,
      "grad_norm": 47.2532844543457,
      "learning_rate": 3.108780661215784e-05,
      "loss": 1.9656,
      "step": 1750
    },
    {
      "epoch": 0.6398862424457874,
      "grad_norm": 35.42753601074219,
      "learning_rate": 3.197653750444366e-05,
      "loss": 2.0286,
      "step": 1800
    },
    {
      "epoch": 0.6576608602915037,
      "grad_norm": 32.47238540649414,
      "learning_rate": 3.286526839672947e-05,
      "loss": 1.8215,
      "step": 1850
    },
    {
      "epoch": 0.67543547813722,
      "grad_norm": 40.022151947021484,
      "learning_rate": 3.3753999289015285e-05,
      "loss": 1.8714,
      "step": 1900
    },
    {
      "epoch": 0.6932100959829364,
      "grad_norm": 39.149559020996094,
      "learning_rate": 3.46427301813011e-05,
      "loss": 1.9406,
      "step": 1950
    },
    {
      "epoch": 0.7109847138286527,
      "grad_norm": 49.03817367553711,
      "learning_rate": 3.553146107358692e-05,
      "loss": 1.8984,
      "step": 2000
    },
    {
      "epoch": 0.728759331674369,
      "grad_norm": 30.21510124206543,
      "learning_rate": 3.6420191965872736e-05,
      "loss": 1.9127,
      "step": 2050
    },
    {
      "epoch": 0.7465339495200853,
      "grad_norm": 33.946224212646484,
      "learning_rate": 3.7308922858158554e-05,
      "loss": 1.9328,
      "step": 2100
    },
    {
      "epoch": 0.7643085673658017,
      "grad_norm": 38.04575729370117,
      "learning_rate": 3.819765375044437e-05,
      "loss": 1.8885,
      "step": 2150
    },
    {
      "epoch": 0.782083185211518,
      "grad_norm": 42.14911651611328,
      "learning_rate": 3.908638464273018e-05,
      "loss": 1.8715,
      "step": 2200
    },
    {
      "epoch": 0.7998578030572343,
      "grad_norm": 34.796913146972656,
      "learning_rate": 3.9975115535016e-05,
      "loss": 1.9713,
      "step": 2250
    },
    {
      "epoch": 0.8176324209029506,
      "grad_norm": 35.36799240112305,
      "learning_rate": 4.0863846427301816e-05,
      "loss": 1.9537,
      "step": 2300
    },
    {
      "epoch": 0.835407038748667,
      "grad_norm": 31.833845138549805,
      "learning_rate": 4.175257731958763e-05,
      "loss": 1.986,
      "step": 2350
    },
    {
      "epoch": 0.8531816565943833,
      "grad_norm": 31.480195999145508,
      "learning_rate": 4.264130821187345e-05,
      "loss": 1.9089,
      "step": 2400
    },
    {
      "epoch": 0.8709562744400995,
      "grad_norm": 28.059268951416016,
      "learning_rate": 4.353003910415927e-05,
      "loss": 1.9828,
      "step": 2450
    },
    {
      "epoch": 0.8887308922858158,
      "grad_norm": 33.97496032714844,
      "learning_rate": 4.4418769996445084e-05,
      "loss": 1.9426,
      "step": 2500
    },
    {
      "epoch": 0.9065055101315321,
      "grad_norm": 30.06556510925293,
      "learning_rate": 4.5307500888730895e-05,
      "loss": 1.8499,
      "step": 2550
    },
    {
      "epoch": 0.9242801279772485,
      "grad_norm": 21.399444580078125,
      "learning_rate": 4.6196231781016705e-05,
      "loss": 1.8867,
      "step": 2600
    },
    {
      "epoch": 0.9420547458229648,
      "grad_norm": 31.51189613342285,
      "learning_rate": 4.708496267330252e-05,
      "loss": 2.0231,
      "step": 2650
    },
    {
      "epoch": 0.9598293636686811,
      "grad_norm": 35.6729850769043,
      "learning_rate": 4.797369356558834e-05,
      "loss": 1.836,
      "step": 2700
    },
    {
      "epoch": 0.9776039815143974,
      "grad_norm": 28.56304931640625,
      "learning_rate": 4.886242445787416e-05,
      "loss": 1.9399,
      "step": 2750
    },
    {
      "epoch": 0.9953785993601137,
      "grad_norm": 39.50929641723633,
      "learning_rate": 4.9751155350159974e-05,
      "loss": 1.8985,
      "step": 2800
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.8892890214920044,
      "eval_runtime": 579.9912,
      "eval_samples_per_second": 8.621,
      "eval_steps_per_second": 0.54,
      "step": 2813
    },
    {
      "epoch": 1.0131532172058302,
      "grad_norm": 25.659931182861328,
      "learning_rate": 4.992890152861714e-05,
      "loss": 1.7091,
      "step": 2850
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 30.346956253051758,
      "learning_rate": 4.983015365169649e-05,
      "loss": 1.7489,
      "step": 2900
    },
    {
      "epoch": 1.0487024528972626,
      "grad_norm": 34.13132095336914,
      "learning_rate": 4.973140577477585e-05,
      "loss": 1.6874,
      "step": 2950
    },
    {
      "epoch": 1.066477070742979,
      "grad_norm": 26.21145248413086,
      "learning_rate": 4.96326578978552e-05,
      "loss": 1.7069,
      "step": 3000
    },
    {
      "epoch": 1.0842516885886953,
      "grad_norm": 37.5620231628418,
      "learning_rate": 4.953391002093455e-05,
      "loss": 1.6872,
      "step": 3050
    },
    {
      "epoch": 1.1020263064344116,
      "grad_norm": 34.38589096069336,
      "learning_rate": 4.943516214401391e-05,
      "loss": 1.636,
      "step": 3100
    },
    {
      "epoch": 1.119800924280128,
      "grad_norm": 22.090560913085938,
      "learning_rate": 4.933641426709326e-05,
      "loss": 1.7174,
      "step": 3150
    },
    {
      "epoch": 1.1375755421258442,
      "grad_norm": 41.0079231262207,
      "learning_rate": 4.923766639017261e-05,
      "loss": 1.7912,
      "step": 3200
    },
    {
      "epoch": 1.1553501599715605,
      "grad_norm": 22.199108123779297,
      "learning_rate": 4.913891851325197e-05,
      "loss": 1.6326,
      "step": 3250
    },
    {
      "epoch": 1.1731247778172769,
      "grad_norm": 27.446725845336914,
      "learning_rate": 4.904017063633132e-05,
      "loss": 1.5657,
      "step": 3300
    },
    {
      "epoch": 1.1908993956629932,
      "grad_norm": 35.660987854003906,
      "learning_rate": 4.8941422759410674e-05,
      "loss": 1.6159,
      "step": 3350
    },
    {
      "epoch": 1.2086740135087095,
      "grad_norm": 34.567176818847656,
      "learning_rate": 4.884267488249003e-05,
      "loss": 1.5432,
      "step": 3400
    },
    {
      "epoch": 1.2264486313544258,
      "grad_norm": 33.95265579223633,
      "learning_rate": 4.8743927005569384e-05,
      "loss": 1.5938,
      "step": 3450
    },
    {
      "epoch": 1.2442232492001422,
      "grad_norm": 28.62511444091797,
      "learning_rate": 4.8645179128648735e-05,
      "loss": 1.6075,
      "step": 3500
    },
    {
      "epoch": 1.2619978670458585,
      "grad_norm": 24.037513732910156,
      "learning_rate": 4.854643125172809e-05,
      "loss": 1.7417,
      "step": 3550
    },
    {
      "epoch": 1.2797724848915748,
      "grad_norm": 29.832693099975586,
      "learning_rate": 4.8447683374807445e-05,
      "loss": 1.607,
      "step": 3600
    },
    {
      "epoch": 1.2975471027372911,
      "grad_norm": 28.9856014251709,
      "learning_rate": 4.8348935497886796e-05,
      "loss": 1.7013,
      "step": 3650
    },
    {
      "epoch": 1.3153217205830074,
      "grad_norm": 29.250324249267578,
      "learning_rate": 4.8250187620966154e-05,
      "loss": 1.6844,
      "step": 3700
    },
    {
      "epoch": 1.3330963384287238,
      "grad_norm": 28.722253799438477,
      "learning_rate": 4.8151439744045506e-05,
      "loss": 1.6514,
      "step": 3750
    },
    {
      "epoch": 1.35087095627444,
      "grad_norm": 27.43202018737793,
      "learning_rate": 4.8052691867124864e-05,
      "loss": 1.6441,
      "step": 3800
    },
    {
      "epoch": 1.3686455741201564,
      "grad_norm": 30.47803497314453,
      "learning_rate": 4.7953943990204215e-05,
      "loss": 1.6084,
      "step": 3850
    },
    {
      "epoch": 1.3864201919658727,
      "grad_norm": 31.406126022338867,
      "learning_rate": 4.7855196113283567e-05,
      "loss": 1.6217,
      "step": 3900
    },
    {
      "epoch": 1.404194809811589,
      "grad_norm": 26.67877769470215,
      "learning_rate": 4.7756448236362925e-05,
      "loss": 1.6372,
      "step": 3950
    },
    {
      "epoch": 1.4219694276573054,
      "grad_norm": 22.837480545043945,
      "learning_rate": 4.7657700359442276e-05,
      "loss": 1.6203,
      "step": 4000
    },
    {
      "epoch": 1.4397440455030217,
      "grad_norm": 24.148517608642578,
      "learning_rate": 4.755895248252163e-05,
      "loss": 1.4567,
      "step": 4050
    },
    {
      "epoch": 1.457518663348738,
      "grad_norm": 37.102542877197266,
      "learning_rate": 4.7460204605600986e-05,
      "loss": 1.5845,
      "step": 4100
    },
    {
      "epoch": 1.4752932811944544,
      "grad_norm": 31.200496673583984,
      "learning_rate": 4.736145672868034e-05,
      "loss": 1.7188,
      "step": 4150
    },
    {
      "epoch": 1.4930678990401707,
      "grad_norm": 30.396106719970703,
      "learning_rate": 4.726270885175969e-05,
      "loss": 1.5597,
      "step": 4200
    },
    {
      "epoch": 1.510842516885887,
      "grad_norm": 23.89527130126953,
      "learning_rate": 4.716396097483905e-05,
      "loss": 1.5111,
      "step": 4250
    },
    {
      "epoch": 1.5286171347316033,
      "grad_norm": 23.669231414794922,
      "learning_rate": 4.70652130979184e-05,
      "loss": 1.5706,
      "step": 4300
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 42.952919006347656,
      "learning_rate": 4.696646522099775e-05,
      "loss": 1.5084,
      "step": 4350
    },
    {
      "epoch": 1.564166370423036,
      "grad_norm": 29.395689010620117,
      "learning_rate": 4.686771734407711e-05,
      "loss": 1.4579,
      "step": 4400
    },
    {
      "epoch": 1.581940988268752,
      "grad_norm": 20.729915618896484,
      "learning_rate": 4.676896946715646e-05,
      "loss": 1.5591,
      "step": 4450
    },
    {
      "epoch": 1.5997156061144686,
      "grad_norm": 29.782583236694336,
      "learning_rate": 4.667022159023581e-05,
      "loss": 1.543,
      "step": 4500
    },
    {
      "epoch": 1.6174902239601847,
      "grad_norm": 22.65672492980957,
      "learning_rate": 4.657147371331517e-05,
      "loss": 1.4308,
      "step": 4550
    },
    {
      "epoch": 1.6352648418059013,
      "grad_norm": 20.578487396240234,
      "learning_rate": 4.647272583639452e-05,
      "loss": 1.5152,
      "step": 4600
    },
    {
      "epoch": 1.6530394596516174,
      "grad_norm": 21.168766021728516,
      "learning_rate": 4.637397795947387e-05,
      "loss": 1.5689,
      "step": 4650
    },
    {
      "epoch": 1.670814077497334,
      "grad_norm": 27.511762619018555,
      "learning_rate": 4.627523008255323e-05,
      "loss": 1.5315,
      "step": 4700
    },
    {
      "epoch": 1.68858869534305,
      "grad_norm": 26.148204803466797,
      "learning_rate": 4.617648220563258e-05,
      "loss": 1.4508,
      "step": 4750
    },
    {
      "epoch": 1.7063633131887666,
      "grad_norm": 19.7584171295166,
      "learning_rate": 4.607773432871194e-05,
      "loss": 1.6017,
      "step": 4800
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 27.02511215209961,
      "learning_rate": 4.597898645179129e-05,
      "loss": 1.426,
      "step": 4850
    },
    {
      "epoch": 1.7419125488801992,
      "grad_norm": 19.026912689208984,
      "learning_rate": 4.588023857487064e-05,
      "loss": 1.4515,
      "step": 4900
    },
    {
      "epoch": 1.7596871667259153,
      "grad_norm": 19.706106185913086,
      "learning_rate": 4.578149069795e-05,
      "loss": 1.4625,
      "step": 4950
    },
    {
      "epoch": 1.7774617845716318,
      "grad_norm": 24.357330322265625,
      "learning_rate": 4.568274282102935e-05,
      "loss": 1.5009,
      "step": 5000
    },
    {
      "epoch": 1.795236402417348,
      "grad_norm": 19.488529205322266,
      "learning_rate": 4.55839949441087e-05,
      "loss": 1.5199,
      "step": 5050
    },
    {
      "epoch": 1.8130110202630645,
      "grad_norm": 19.253864288330078,
      "learning_rate": 4.548524706718806e-05,
      "loss": 1.5089,
      "step": 5100
    },
    {
      "epoch": 1.8307856381087806,
      "grad_norm": 26.04707145690918,
      "learning_rate": 4.538649919026741e-05,
      "loss": 1.4728,
      "step": 5150
    },
    {
      "epoch": 1.8485602559544971,
      "grad_norm": 31.742950439453125,
      "learning_rate": 4.5287751313346764e-05,
      "loss": 1.5536,
      "step": 5200
    },
    {
      "epoch": 1.8663348738002132,
      "grad_norm": 25.96784210205078,
      "learning_rate": 4.518900343642612e-05,
      "loss": 1.4554,
      "step": 5250
    },
    {
      "epoch": 1.8841094916459296,
      "grad_norm": 22.465164184570312,
      "learning_rate": 4.509025555950547e-05,
      "loss": 1.5746,
      "step": 5300
    },
    {
      "epoch": 1.9018841094916459,
      "grad_norm": 26.8048152923584,
      "learning_rate": 4.4991507682584825e-05,
      "loss": 1.4705,
      "step": 5350
    },
    {
      "epoch": 1.9196587273373622,
      "grad_norm": 27.170921325683594,
      "learning_rate": 4.489275980566418e-05,
      "loss": 1.5308,
      "step": 5400
    },
    {
      "epoch": 1.9374333451830785,
      "grad_norm": 22.280746459960938,
      "learning_rate": 4.4794011928743534e-05,
      "loss": 1.4,
      "step": 5450
    },
    {
      "epoch": 1.9552079630287948,
      "grad_norm": 16.36634635925293,
      "learning_rate": 4.4695264051822886e-05,
      "loss": 1.422,
      "step": 5500
    },
    {
      "epoch": 1.9729825808745112,
      "grad_norm": 18.951019287109375,
      "learning_rate": 4.4596516174902244e-05,
      "loss": 1.3907,
      "step": 5550
    },
    {
      "epoch": 1.9907571987202275,
      "grad_norm": 27.008665084838867,
      "learning_rate": 4.4497768297981595e-05,
      "loss": 1.4914,
      "step": 5600
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.5214927196502686,
      "eval_runtime": 569.0679,
      "eval_samples_per_second": 8.786,
      "eval_steps_per_second": 0.55,
      "step": 5626
    }
  ],
  "logging_steps": 50,
  "max_steps": 28130,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.54511758993082e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
