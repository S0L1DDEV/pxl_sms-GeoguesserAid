{
  "best_global_step": 8439,
  "best_metric": 1.3499529361724854,
  "best_model_checkpoint": "./checkpoints\\checkpoint-8439",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 28130,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017774617845716316,
      "grad_norm": 45.72555160522461,
      "learning_rate": 8.709562744400995e-07,
      "loss": 5.1679,
      "step": 50
    },
    {
      "epoch": 0.03554923569143263,
      "grad_norm": 37.387176513671875,
      "learning_rate": 1.7596871667259156e-06,
      "loss": 4.3626,
      "step": 100
    },
    {
      "epoch": 0.053323853537148955,
      "grad_norm": 39.64204788208008,
      "learning_rate": 2.6484180590117313e-06,
      "loss": 3.6069,
      "step": 150
    },
    {
      "epoch": 0.07109847138286526,
      "grad_norm": 40.33796310424805,
      "learning_rate": 3.5371489512975476e-06,
      "loss": 3.3746,
      "step": 200
    },
    {
      "epoch": 0.08887308922858159,
      "grad_norm": 44.7415657043457,
      "learning_rate": 4.425879843583363e-06,
      "loss": 3.0482,
      "step": 250
    },
    {
      "epoch": 0.10664770707429791,
      "grad_norm": 66.34918975830078,
      "learning_rate": 5.314610735869179e-06,
      "loss": 2.9366,
      "step": 300
    },
    {
      "epoch": 0.12442232492001422,
      "grad_norm": 57.899330139160156,
      "learning_rate": 6.2033416281549945e-06,
      "loss": 2.7185,
      "step": 350
    },
    {
      "epoch": 0.14219694276573053,
      "grad_norm": 56.05861282348633,
      "learning_rate": 7.092072520440811e-06,
      "loss": 2.6071,
      "step": 400
    },
    {
      "epoch": 0.15997156061144685,
      "grad_norm": 59.65100860595703,
      "learning_rate": 7.980803412726627e-06,
      "loss": 2.4038,
      "step": 450
    },
    {
      "epoch": 0.17774617845716317,
      "grad_norm": 59.519691467285156,
      "learning_rate": 8.869534305012443e-06,
      "loss": 2.4168,
      "step": 500
    },
    {
      "epoch": 0.1955207963028795,
      "grad_norm": 51.024356842041016,
      "learning_rate": 9.758265197298258e-06,
      "loss": 2.3297,
      "step": 550
    },
    {
      "epoch": 0.21329541414859582,
      "grad_norm": 51.21497344970703,
      "learning_rate": 1.0646996089584074e-05,
      "loss": 2.1683,
      "step": 600
    },
    {
      "epoch": 0.23107003199431211,
      "grad_norm": 55.06470489501953,
      "learning_rate": 1.1535726981869891e-05,
      "loss": 2.199,
      "step": 650
    },
    {
      "epoch": 0.24884464984002844,
      "grad_norm": 53.37484359741211,
      "learning_rate": 1.2424457874155706e-05,
      "loss": 2.1537,
      "step": 700
    },
    {
      "epoch": 0.26661926768574473,
      "grad_norm": 43.10935974121094,
      "learning_rate": 1.3313188766441524e-05,
      "loss": 2.0608,
      "step": 750
    },
    {
      "epoch": 0.28439388553146105,
      "grad_norm": 57.46440887451172,
      "learning_rate": 1.4201919658727339e-05,
      "loss": 2.036,
      "step": 800
    },
    {
      "epoch": 0.3021685033771774,
      "grad_norm": 37.48624801635742,
      "learning_rate": 1.5090650551013155e-05,
      "loss": 2.0457,
      "step": 850
    },
    {
      "epoch": 0.3199431212228937,
      "grad_norm": 53.04494094848633,
      "learning_rate": 1.5979381443298968e-05,
      "loss": 1.8508,
      "step": 900
    },
    {
      "epoch": 0.33771773906861,
      "grad_norm": 68.19896697998047,
      "learning_rate": 1.6868112335584785e-05,
      "loss": 1.9957,
      "step": 950
    },
    {
      "epoch": 0.35549235691432635,
      "grad_norm": 70.56356811523438,
      "learning_rate": 1.77568432278706e-05,
      "loss": 1.9539,
      "step": 1000
    },
    {
      "epoch": 0.37326697476004267,
      "grad_norm": 49.84227752685547,
      "learning_rate": 1.8645574120156416e-05,
      "loss": 1.8547,
      "step": 1050
    },
    {
      "epoch": 0.391041592605759,
      "grad_norm": 43.23459243774414,
      "learning_rate": 1.9534305012442234e-05,
      "loss": 1.886,
      "step": 1100
    },
    {
      "epoch": 0.4088162104514753,
      "grad_norm": 42.602691650390625,
      "learning_rate": 2.042303590472805e-05,
      "loss": 2.0184,
      "step": 1150
    },
    {
      "epoch": 0.42659082829719164,
      "grad_norm": 37.47589874267578,
      "learning_rate": 2.1311766797013865e-05,
      "loss": 1.792,
      "step": 1200
    },
    {
      "epoch": 0.4443654461429079,
      "grad_norm": 57.8959846496582,
      "learning_rate": 2.2200497689299682e-05,
      "loss": 2.0696,
      "step": 1250
    },
    {
      "epoch": 0.46214006398862423,
      "grad_norm": 44.53250503540039,
      "learning_rate": 2.30892285815855e-05,
      "loss": 1.7371,
      "step": 1300
    },
    {
      "epoch": 0.47991468183434055,
      "grad_norm": 37.292388916015625,
      "learning_rate": 2.3977959473871313e-05,
      "loss": 1.9644,
      "step": 1350
    },
    {
      "epoch": 0.4976892996800569,
      "grad_norm": 51.556644439697266,
      "learning_rate": 2.4866690366157127e-05,
      "loss": 1.8969,
      "step": 1400
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 46.945472717285156,
      "learning_rate": 2.5755421258442947e-05,
      "loss": 1.9427,
      "step": 1450
    },
    {
      "epoch": 0.5332385353714895,
      "grad_norm": 53.55520248413086,
      "learning_rate": 2.6644152150728764e-05,
      "loss": 1.9012,
      "step": 1500
    },
    {
      "epoch": 0.5510131532172058,
      "grad_norm": 34.33576583862305,
      "learning_rate": 2.7532883043014578e-05,
      "loss": 1.8628,
      "step": 1550
    },
    {
      "epoch": 0.5687877710629221,
      "grad_norm": 35.07330322265625,
      "learning_rate": 2.8421613935300395e-05,
      "loss": 1.9381,
      "step": 1600
    },
    {
      "epoch": 0.5865623889086384,
      "grad_norm": 32.17849349975586,
      "learning_rate": 2.9310344827586206e-05,
      "loss": 1.8474,
      "step": 1650
    },
    {
      "epoch": 0.6043370067543548,
      "grad_norm": 40.79207229614258,
      "learning_rate": 3.0199075719872023e-05,
      "loss": 1.8367,
      "step": 1700
    },
    {
      "epoch": 0.6221116246000711,
      "grad_norm": 47.2532844543457,
      "learning_rate": 3.108780661215784e-05,
      "loss": 1.9656,
      "step": 1750
    },
    {
      "epoch": 0.6398862424457874,
      "grad_norm": 35.42753601074219,
      "learning_rate": 3.197653750444366e-05,
      "loss": 2.0286,
      "step": 1800
    },
    {
      "epoch": 0.6576608602915037,
      "grad_norm": 32.47238540649414,
      "learning_rate": 3.286526839672947e-05,
      "loss": 1.8215,
      "step": 1850
    },
    {
      "epoch": 0.67543547813722,
      "grad_norm": 40.022151947021484,
      "learning_rate": 3.3753999289015285e-05,
      "loss": 1.8714,
      "step": 1900
    },
    {
      "epoch": 0.6932100959829364,
      "grad_norm": 39.149559020996094,
      "learning_rate": 3.46427301813011e-05,
      "loss": 1.9406,
      "step": 1950
    },
    {
      "epoch": 0.7109847138286527,
      "grad_norm": 49.03817367553711,
      "learning_rate": 3.553146107358692e-05,
      "loss": 1.8984,
      "step": 2000
    },
    {
      "epoch": 0.728759331674369,
      "grad_norm": 30.21510124206543,
      "learning_rate": 3.6420191965872736e-05,
      "loss": 1.9127,
      "step": 2050
    },
    {
      "epoch": 0.7465339495200853,
      "grad_norm": 33.946224212646484,
      "learning_rate": 3.7308922858158554e-05,
      "loss": 1.9328,
      "step": 2100
    },
    {
      "epoch": 0.7643085673658017,
      "grad_norm": 38.04575729370117,
      "learning_rate": 3.819765375044437e-05,
      "loss": 1.8885,
      "step": 2150
    },
    {
      "epoch": 0.782083185211518,
      "grad_norm": 42.14911651611328,
      "learning_rate": 3.908638464273018e-05,
      "loss": 1.8715,
      "step": 2200
    },
    {
      "epoch": 0.7998578030572343,
      "grad_norm": 34.796913146972656,
      "learning_rate": 3.9975115535016e-05,
      "loss": 1.9713,
      "step": 2250
    },
    {
      "epoch": 0.8176324209029506,
      "grad_norm": 35.36799240112305,
      "learning_rate": 4.0863846427301816e-05,
      "loss": 1.9537,
      "step": 2300
    },
    {
      "epoch": 0.835407038748667,
      "grad_norm": 31.833845138549805,
      "learning_rate": 4.175257731958763e-05,
      "loss": 1.986,
      "step": 2350
    },
    {
      "epoch": 0.8531816565943833,
      "grad_norm": 31.480195999145508,
      "learning_rate": 4.264130821187345e-05,
      "loss": 1.9089,
      "step": 2400
    },
    {
      "epoch": 0.8709562744400995,
      "grad_norm": 28.059268951416016,
      "learning_rate": 4.353003910415927e-05,
      "loss": 1.9828,
      "step": 2450
    },
    {
      "epoch": 0.8887308922858158,
      "grad_norm": 33.97496032714844,
      "learning_rate": 4.4418769996445084e-05,
      "loss": 1.9426,
      "step": 2500
    },
    {
      "epoch": 0.9065055101315321,
      "grad_norm": 30.06556510925293,
      "learning_rate": 4.5307500888730895e-05,
      "loss": 1.8499,
      "step": 2550
    },
    {
      "epoch": 0.9242801279772485,
      "grad_norm": 21.399444580078125,
      "learning_rate": 4.6196231781016705e-05,
      "loss": 1.8867,
      "step": 2600
    },
    {
      "epoch": 0.9420547458229648,
      "grad_norm": 31.51189613342285,
      "learning_rate": 4.708496267330252e-05,
      "loss": 2.0231,
      "step": 2650
    },
    {
      "epoch": 0.9598293636686811,
      "grad_norm": 35.6729850769043,
      "learning_rate": 4.797369356558834e-05,
      "loss": 1.836,
      "step": 2700
    },
    {
      "epoch": 0.9776039815143974,
      "grad_norm": 28.56304931640625,
      "learning_rate": 4.886242445787416e-05,
      "loss": 1.9399,
      "step": 2750
    },
    {
      "epoch": 0.9953785993601137,
      "grad_norm": 39.50929641723633,
      "learning_rate": 4.9751155350159974e-05,
      "loss": 1.8985,
      "step": 2800
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.8892890214920044,
      "eval_runtime": 579.9912,
      "eval_samples_per_second": 8.621,
      "eval_steps_per_second": 0.54,
      "step": 2813
    },
    {
      "epoch": 1.0131532172058302,
      "grad_norm": 25.659931182861328,
      "learning_rate": 4.992890152861714e-05,
      "loss": 1.7091,
      "step": 2850
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 30.346956253051758,
      "learning_rate": 4.983015365169649e-05,
      "loss": 1.7489,
      "step": 2900
    },
    {
      "epoch": 1.0487024528972626,
      "grad_norm": 34.13132095336914,
      "learning_rate": 4.973140577477585e-05,
      "loss": 1.6874,
      "step": 2950
    },
    {
      "epoch": 1.066477070742979,
      "grad_norm": 26.21145248413086,
      "learning_rate": 4.96326578978552e-05,
      "loss": 1.7069,
      "step": 3000
    },
    {
      "epoch": 1.0842516885886953,
      "grad_norm": 37.5620231628418,
      "learning_rate": 4.953391002093455e-05,
      "loss": 1.6872,
      "step": 3050
    },
    {
      "epoch": 1.1020263064344116,
      "grad_norm": 34.38589096069336,
      "learning_rate": 4.943516214401391e-05,
      "loss": 1.636,
      "step": 3100
    },
    {
      "epoch": 1.119800924280128,
      "grad_norm": 22.090560913085938,
      "learning_rate": 4.933641426709326e-05,
      "loss": 1.7174,
      "step": 3150
    },
    {
      "epoch": 1.1375755421258442,
      "grad_norm": 41.0079231262207,
      "learning_rate": 4.923766639017261e-05,
      "loss": 1.7912,
      "step": 3200
    },
    {
      "epoch": 1.1553501599715605,
      "grad_norm": 22.199108123779297,
      "learning_rate": 4.913891851325197e-05,
      "loss": 1.6326,
      "step": 3250
    },
    {
      "epoch": 1.1731247778172769,
      "grad_norm": 27.446725845336914,
      "learning_rate": 4.904017063633132e-05,
      "loss": 1.5657,
      "step": 3300
    },
    {
      "epoch": 1.1908993956629932,
      "grad_norm": 35.660987854003906,
      "learning_rate": 4.8941422759410674e-05,
      "loss": 1.6159,
      "step": 3350
    },
    {
      "epoch": 1.2086740135087095,
      "grad_norm": 34.567176818847656,
      "learning_rate": 4.884267488249003e-05,
      "loss": 1.5432,
      "step": 3400
    },
    {
      "epoch": 1.2264486313544258,
      "grad_norm": 33.95265579223633,
      "learning_rate": 4.8743927005569384e-05,
      "loss": 1.5938,
      "step": 3450
    },
    {
      "epoch": 1.2442232492001422,
      "grad_norm": 28.62511444091797,
      "learning_rate": 4.8645179128648735e-05,
      "loss": 1.6075,
      "step": 3500
    },
    {
      "epoch": 1.2619978670458585,
      "grad_norm": 24.037513732910156,
      "learning_rate": 4.854643125172809e-05,
      "loss": 1.7417,
      "step": 3550
    },
    {
      "epoch": 1.2797724848915748,
      "grad_norm": 29.832693099975586,
      "learning_rate": 4.8447683374807445e-05,
      "loss": 1.607,
      "step": 3600
    },
    {
      "epoch": 1.2975471027372911,
      "grad_norm": 28.9856014251709,
      "learning_rate": 4.8348935497886796e-05,
      "loss": 1.7013,
      "step": 3650
    },
    {
      "epoch": 1.3153217205830074,
      "grad_norm": 29.250324249267578,
      "learning_rate": 4.8250187620966154e-05,
      "loss": 1.6844,
      "step": 3700
    },
    {
      "epoch": 1.3330963384287238,
      "grad_norm": 28.722253799438477,
      "learning_rate": 4.8151439744045506e-05,
      "loss": 1.6514,
      "step": 3750
    },
    {
      "epoch": 1.35087095627444,
      "grad_norm": 27.43202018737793,
      "learning_rate": 4.8052691867124864e-05,
      "loss": 1.6441,
      "step": 3800
    },
    {
      "epoch": 1.3686455741201564,
      "grad_norm": 30.47803497314453,
      "learning_rate": 4.7953943990204215e-05,
      "loss": 1.6084,
      "step": 3850
    },
    {
      "epoch": 1.3864201919658727,
      "grad_norm": 31.406126022338867,
      "learning_rate": 4.7855196113283567e-05,
      "loss": 1.6217,
      "step": 3900
    },
    {
      "epoch": 1.404194809811589,
      "grad_norm": 26.67877769470215,
      "learning_rate": 4.7756448236362925e-05,
      "loss": 1.6372,
      "step": 3950
    },
    {
      "epoch": 1.4219694276573054,
      "grad_norm": 22.837480545043945,
      "learning_rate": 4.7657700359442276e-05,
      "loss": 1.6203,
      "step": 4000
    },
    {
      "epoch": 1.4397440455030217,
      "grad_norm": 24.148517608642578,
      "learning_rate": 4.755895248252163e-05,
      "loss": 1.4567,
      "step": 4050
    },
    {
      "epoch": 1.457518663348738,
      "grad_norm": 37.102542877197266,
      "learning_rate": 4.7460204605600986e-05,
      "loss": 1.5845,
      "step": 4100
    },
    {
      "epoch": 1.4752932811944544,
      "grad_norm": 31.200496673583984,
      "learning_rate": 4.736145672868034e-05,
      "loss": 1.7188,
      "step": 4150
    },
    {
      "epoch": 1.4930678990401707,
      "grad_norm": 30.396106719970703,
      "learning_rate": 4.726270885175969e-05,
      "loss": 1.5597,
      "step": 4200
    },
    {
      "epoch": 1.510842516885887,
      "grad_norm": 23.89527130126953,
      "learning_rate": 4.716396097483905e-05,
      "loss": 1.5111,
      "step": 4250
    },
    {
      "epoch": 1.5286171347316033,
      "grad_norm": 23.669231414794922,
      "learning_rate": 4.70652130979184e-05,
      "loss": 1.5706,
      "step": 4300
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 42.952919006347656,
      "learning_rate": 4.696646522099775e-05,
      "loss": 1.5084,
      "step": 4350
    },
    {
      "epoch": 1.564166370423036,
      "grad_norm": 29.395689010620117,
      "learning_rate": 4.686771734407711e-05,
      "loss": 1.4579,
      "step": 4400
    },
    {
      "epoch": 1.581940988268752,
      "grad_norm": 20.729915618896484,
      "learning_rate": 4.676896946715646e-05,
      "loss": 1.5591,
      "step": 4450
    },
    {
      "epoch": 1.5997156061144686,
      "grad_norm": 29.782583236694336,
      "learning_rate": 4.667022159023581e-05,
      "loss": 1.543,
      "step": 4500
    },
    {
      "epoch": 1.6174902239601847,
      "grad_norm": 22.65672492980957,
      "learning_rate": 4.657147371331517e-05,
      "loss": 1.4308,
      "step": 4550
    },
    {
      "epoch": 1.6352648418059013,
      "grad_norm": 20.578487396240234,
      "learning_rate": 4.647272583639452e-05,
      "loss": 1.5152,
      "step": 4600
    },
    {
      "epoch": 1.6530394596516174,
      "grad_norm": 21.168766021728516,
      "learning_rate": 4.637397795947387e-05,
      "loss": 1.5689,
      "step": 4650
    },
    {
      "epoch": 1.670814077497334,
      "grad_norm": 27.511762619018555,
      "learning_rate": 4.627523008255323e-05,
      "loss": 1.5315,
      "step": 4700
    },
    {
      "epoch": 1.68858869534305,
      "grad_norm": 26.148204803466797,
      "learning_rate": 4.617648220563258e-05,
      "loss": 1.4508,
      "step": 4750
    },
    {
      "epoch": 1.7063633131887666,
      "grad_norm": 19.7584171295166,
      "learning_rate": 4.607773432871194e-05,
      "loss": 1.6017,
      "step": 4800
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 27.02511215209961,
      "learning_rate": 4.597898645179129e-05,
      "loss": 1.426,
      "step": 4850
    },
    {
      "epoch": 1.7419125488801992,
      "grad_norm": 19.026912689208984,
      "learning_rate": 4.588023857487064e-05,
      "loss": 1.4515,
      "step": 4900
    },
    {
      "epoch": 1.7596871667259153,
      "grad_norm": 19.706106185913086,
      "learning_rate": 4.578149069795e-05,
      "loss": 1.4625,
      "step": 4950
    },
    {
      "epoch": 1.7774617845716318,
      "grad_norm": 24.357330322265625,
      "learning_rate": 4.568274282102935e-05,
      "loss": 1.5009,
      "step": 5000
    },
    {
      "epoch": 1.795236402417348,
      "grad_norm": 19.488529205322266,
      "learning_rate": 4.55839949441087e-05,
      "loss": 1.5199,
      "step": 5050
    },
    {
      "epoch": 1.8130110202630645,
      "grad_norm": 19.253864288330078,
      "learning_rate": 4.548524706718806e-05,
      "loss": 1.5089,
      "step": 5100
    },
    {
      "epoch": 1.8307856381087806,
      "grad_norm": 26.04707145690918,
      "learning_rate": 4.538649919026741e-05,
      "loss": 1.4728,
      "step": 5150
    },
    {
      "epoch": 1.8485602559544971,
      "grad_norm": 31.742950439453125,
      "learning_rate": 4.5287751313346764e-05,
      "loss": 1.5536,
      "step": 5200
    },
    {
      "epoch": 1.8663348738002132,
      "grad_norm": 25.96784210205078,
      "learning_rate": 4.518900343642612e-05,
      "loss": 1.4554,
      "step": 5250
    },
    {
      "epoch": 1.8841094916459296,
      "grad_norm": 22.465164184570312,
      "learning_rate": 4.509025555950547e-05,
      "loss": 1.5746,
      "step": 5300
    },
    {
      "epoch": 1.9018841094916459,
      "grad_norm": 26.8048152923584,
      "learning_rate": 4.4991507682584825e-05,
      "loss": 1.4705,
      "step": 5350
    },
    {
      "epoch": 1.9196587273373622,
      "grad_norm": 27.170921325683594,
      "learning_rate": 4.489275980566418e-05,
      "loss": 1.5308,
      "step": 5400
    },
    {
      "epoch": 1.9374333451830785,
      "grad_norm": 22.280746459960938,
      "learning_rate": 4.4794011928743534e-05,
      "loss": 1.4,
      "step": 5450
    },
    {
      "epoch": 1.9552079630287948,
      "grad_norm": 16.36634635925293,
      "learning_rate": 4.4695264051822886e-05,
      "loss": 1.422,
      "step": 5500
    },
    {
      "epoch": 1.9729825808745112,
      "grad_norm": 18.951019287109375,
      "learning_rate": 4.4596516174902244e-05,
      "loss": 1.3907,
      "step": 5550
    },
    {
      "epoch": 1.9907571987202275,
      "grad_norm": 27.008665084838867,
      "learning_rate": 4.4497768297981595e-05,
      "loss": 1.4914,
      "step": 5600
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.5214927196502686,
      "eval_runtime": 569.0679,
      "eval_samples_per_second": 8.786,
      "eval_steps_per_second": 0.55,
      "step": 5626
    },
    {
      "epoch": 2.008531816565944,
      "grad_norm": 22.139955520629883,
      "learning_rate": 4.4399020421060946e-05,
      "loss": 1.2334,
      "step": 5650
    },
    {
      "epoch": 2.0263064344116604,
      "grad_norm": 19.473690032958984,
      "learning_rate": 4.4300272544140305e-05,
      "loss": 0.9182,
      "step": 5700
    },
    {
      "epoch": 2.0440810522573765,
      "grad_norm": 31.54852294921875,
      "learning_rate": 4.4201524667219656e-05,
      "loss": 1.1257,
      "step": 5750
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 23.8635196685791,
      "learning_rate": 4.410277679029901e-05,
      "loss": 1.174,
      "step": 5800
    },
    {
      "epoch": 2.079630287948809,
      "grad_norm": 18.713970184326172,
      "learning_rate": 4.4004028913378366e-05,
      "loss": 1.0329,
      "step": 5850
    },
    {
      "epoch": 2.097404905794525,
      "grad_norm": 25.963918685913086,
      "learning_rate": 4.390528103645772e-05,
      "loss": 0.9811,
      "step": 5900
    },
    {
      "epoch": 2.1151795236402418,
      "grad_norm": 17.008834838867188,
      "learning_rate": 4.3806533159537075e-05,
      "loss": 1.02,
      "step": 5950
    },
    {
      "epoch": 2.132954141485958,
      "grad_norm": 23.97748565673828,
      "learning_rate": 4.3707785282616426e-05,
      "loss": 1.0977,
      "step": 6000
    },
    {
      "epoch": 2.1507287593316744,
      "grad_norm": 30.05948829650879,
      "learning_rate": 4.360903740569578e-05,
      "loss": 1.0438,
      "step": 6050
    },
    {
      "epoch": 2.1685033771773905,
      "grad_norm": 33.07263946533203,
      "learning_rate": 4.3510289528775136e-05,
      "loss": 1.111,
      "step": 6100
    },
    {
      "epoch": 2.186277995023107,
      "grad_norm": 43.2642936706543,
      "learning_rate": 4.341154165185449e-05,
      "loss": 0.9793,
      "step": 6150
    },
    {
      "epoch": 2.204052612868823,
      "grad_norm": 23.59796905517578,
      "learning_rate": 4.331279377493384e-05,
      "loss": 1.0023,
      "step": 6200
    },
    {
      "epoch": 2.2218272307145397,
      "grad_norm": 19.624591827392578,
      "learning_rate": 4.32140458980132e-05,
      "loss": 1.1256,
      "step": 6250
    },
    {
      "epoch": 2.239601848560256,
      "grad_norm": 32.2612419128418,
      "learning_rate": 4.311529802109255e-05,
      "loss": 1.0984,
      "step": 6300
    },
    {
      "epoch": 2.2573764664059723,
      "grad_norm": 20.0377254486084,
      "learning_rate": 4.30165501441719e-05,
      "loss": 1.0678,
      "step": 6350
    },
    {
      "epoch": 2.2751510842516884,
      "grad_norm": 29.187519073486328,
      "learning_rate": 4.291780226725126e-05,
      "loss": 1.0049,
      "step": 6400
    },
    {
      "epoch": 2.292925702097405,
      "grad_norm": 17.963415145874023,
      "learning_rate": 4.281905439033061e-05,
      "loss": 1.0687,
      "step": 6450
    },
    {
      "epoch": 2.310700319943121,
      "grad_norm": 26.822805404663086,
      "learning_rate": 4.272030651340996e-05,
      "loss": 1.0682,
      "step": 6500
    },
    {
      "epoch": 2.3284749377888376,
      "grad_norm": 27.647830963134766,
      "learning_rate": 4.262155863648932e-05,
      "loss": 1.0849,
      "step": 6550
    },
    {
      "epoch": 2.3462495556345537,
      "grad_norm": 25.731369018554688,
      "learning_rate": 4.252281075956867e-05,
      "loss": 1.0307,
      "step": 6600
    },
    {
      "epoch": 2.3640241734802703,
      "grad_norm": 25.352020263671875,
      "learning_rate": 4.242406288264802e-05,
      "loss": 1.0412,
      "step": 6650
    },
    {
      "epoch": 2.3817987913259864,
      "grad_norm": 21.92180061340332,
      "learning_rate": 4.232531500572738e-05,
      "loss": 1.0782,
      "step": 6700
    },
    {
      "epoch": 2.399573409171703,
      "grad_norm": 12.70843505859375,
      "learning_rate": 4.222656712880673e-05,
      "loss": 1.075,
      "step": 6750
    },
    {
      "epoch": 2.417348027017419,
      "grad_norm": 24.74819564819336,
      "learning_rate": 4.212781925188608e-05,
      "loss": 1.0111,
      "step": 6800
    },
    {
      "epoch": 2.4351226448631356,
      "grad_norm": 25.528308868408203,
      "learning_rate": 4.202907137496544e-05,
      "loss": 1.0418,
      "step": 6850
    },
    {
      "epoch": 2.4528972627088517,
      "grad_norm": 35.784183502197266,
      "learning_rate": 4.193032349804479e-05,
      "loss": 1.1197,
      "step": 6900
    },
    {
      "epoch": 2.470671880554568,
      "grad_norm": 31.773218154907227,
      "learning_rate": 4.183157562112415e-05,
      "loss": 1.0841,
      "step": 6950
    },
    {
      "epoch": 2.4884464984002843,
      "grad_norm": 20.92281723022461,
      "learning_rate": 4.17328277442035e-05,
      "loss": 1.1036,
      "step": 7000
    },
    {
      "epoch": 2.506221116246001,
      "grad_norm": 25.26148223876953,
      "learning_rate": 4.163407986728285e-05,
      "loss": 1.028,
      "step": 7050
    },
    {
      "epoch": 2.523995734091717,
      "grad_norm": 24.617033004760742,
      "learning_rate": 4.153533199036221e-05,
      "loss": 0.9876,
      "step": 7100
    },
    {
      "epoch": 2.5417703519374335,
      "grad_norm": 26.52050018310547,
      "learning_rate": 4.143658411344156e-05,
      "loss": 1.0748,
      "step": 7150
    },
    {
      "epoch": 2.5595449697831496,
      "grad_norm": 26.701807022094727,
      "learning_rate": 4.1337836236520914e-05,
      "loss": 1.0996,
      "step": 7200
    },
    {
      "epoch": 2.5773195876288657,
      "grad_norm": 48.4124755859375,
      "learning_rate": 4.123908835960027e-05,
      "loss": 1.1379,
      "step": 7250
    },
    {
      "epoch": 2.5950942054745822,
      "grad_norm": 28.421138763427734,
      "learning_rate": 4.1140340482679624e-05,
      "loss": 1.0249,
      "step": 7300
    },
    {
      "epoch": 2.612868823320299,
      "grad_norm": 26.9412841796875,
      "learning_rate": 4.1041592605758975e-05,
      "loss": 0.9653,
      "step": 7350
    },
    {
      "epoch": 2.630643441166015,
      "grad_norm": 19.050764083862305,
      "learning_rate": 4.094284472883833e-05,
      "loss": 1.0202,
      "step": 7400
    },
    {
      "epoch": 2.648418059011731,
      "grad_norm": 19.81344223022461,
      "learning_rate": 4.0844096851917684e-05,
      "loss": 1.0404,
      "step": 7450
    },
    {
      "epoch": 2.6661926768574475,
      "grad_norm": 40.7639274597168,
      "learning_rate": 4.0745348974997036e-05,
      "loss": 0.9884,
      "step": 7500
    },
    {
      "epoch": 2.683967294703164,
      "grad_norm": 28.62834358215332,
      "learning_rate": 4.0646601098076394e-05,
      "loss": 1.0541,
      "step": 7550
    },
    {
      "epoch": 2.70174191254888,
      "grad_norm": 29.86837387084961,
      "learning_rate": 4.0547853221155745e-05,
      "loss": 1.0564,
      "step": 7600
    },
    {
      "epoch": 2.7195165303945963,
      "grad_norm": 29.453645706176758,
      "learning_rate": 4.04491053442351e-05,
      "loss": 1.0581,
      "step": 7650
    },
    {
      "epoch": 2.737291148240313,
      "grad_norm": 25.032562255859375,
      "learning_rate": 4.0350357467314455e-05,
      "loss": 0.9444,
      "step": 7700
    },
    {
      "epoch": 2.7550657660860294,
      "grad_norm": 24.643957138061523,
      "learning_rate": 4.0251609590393806e-05,
      "loss": 1.1764,
      "step": 7750
    },
    {
      "epoch": 2.7728403839317455,
      "grad_norm": 21.62310028076172,
      "learning_rate": 4.015286171347316e-05,
      "loss": 1.0026,
      "step": 7800
    },
    {
      "epoch": 2.7906150017774616,
      "grad_norm": 32.50385665893555,
      "learning_rate": 4.0054113836552516e-05,
      "loss": 1.0692,
      "step": 7850
    },
    {
      "epoch": 2.808389619623178,
      "grad_norm": 29.179288864135742,
      "learning_rate": 3.995536595963187e-05,
      "loss": 1.1436,
      "step": 7900
    },
    {
      "epoch": 2.8261642374688947,
      "grad_norm": 17.948240280151367,
      "learning_rate": 3.9856618082711225e-05,
      "loss": 1.0807,
      "step": 7950
    },
    {
      "epoch": 2.8439388553146108,
      "grad_norm": 30.448869705200195,
      "learning_rate": 3.975787020579058e-05,
      "loss": 1.0507,
      "step": 8000
    },
    {
      "epoch": 2.861713473160327,
      "grad_norm": 23.68577003479004,
      "learning_rate": 3.965912232886993e-05,
      "loss": 1.081,
      "step": 8050
    },
    {
      "epoch": 2.8794880910060434,
      "grad_norm": 33.38752365112305,
      "learning_rate": 3.9560374451949286e-05,
      "loss": 0.9865,
      "step": 8100
    },
    {
      "epoch": 2.89726270885176,
      "grad_norm": 21.95579719543457,
      "learning_rate": 3.946162657502864e-05,
      "loss": 1.0675,
      "step": 8150
    },
    {
      "epoch": 2.915037326697476,
      "grad_norm": 32.327880859375,
      "learning_rate": 3.936287869810799e-05,
      "loss": 1.0284,
      "step": 8200
    },
    {
      "epoch": 2.932811944543192,
      "grad_norm": 17.930652618408203,
      "learning_rate": 3.926413082118735e-05,
      "loss": 1.0355,
      "step": 8250
    },
    {
      "epoch": 2.9505865623889087,
      "grad_norm": 34.20661163330078,
      "learning_rate": 3.91653829442667e-05,
      "loss": 1.0186,
      "step": 8300
    },
    {
      "epoch": 2.968361180234625,
      "grad_norm": 27.941930770874023,
      "learning_rate": 3.906663506734605e-05,
      "loss": 1.0054,
      "step": 8350
    },
    {
      "epoch": 2.9861357980803414,
      "grad_norm": 30.37055015563965,
      "learning_rate": 3.896788719042541e-05,
      "loss": 0.9835,
      "step": 8400
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.3499529361724854,
      "eval_runtime": 595.2934,
      "eval_samples_per_second": 8.399,
      "eval_steps_per_second": 0.526,
      "step": 8439
    },
    {
      "epoch": 3.0039104159260575,
      "grad_norm": 20.970138549804688,
      "learning_rate": 3.886913931350476e-05,
      "loss": 0.8875,
      "step": 8450
    },
    {
      "epoch": 3.021685033771774,
      "grad_norm": 14.04200553894043,
      "learning_rate": 3.877039143658411e-05,
      "loss": 0.6099,
      "step": 8500
    },
    {
      "epoch": 3.03945965161749,
      "grad_norm": 18.750411987304688,
      "learning_rate": 3.867164355966347e-05,
      "loss": 0.6424,
      "step": 8550
    },
    {
      "epoch": 3.0572342694632066,
      "grad_norm": 21.11343765258789,
      "learning_rate": 3.857289568274282e-05,
      "loss": 0.5792,
      "step": 8600
    },
    {
      "epoch": 3.0750088873089227,
      "grad_norm": 21.136653900146484,
      "learning_rate": 3.847414780582217e-05,
      "loss": 0.5505,
      "step": 8650
    },
    {
      "epoch": 3.0927835051546393,
      "grad_norm": 25.195865631103516,
      "learning_rate": 3.837539992890153e-05,
      "loss": 0.6108,
      "step": 8700
    },
    {
      "epoch": 3.1105581230003554,
      "grad_norm": 24.272708892822266,
      "learning_rate": 3.827665205198088e-05,
      "loss": 0.6472,
      "step": 8750
    },
    {
      "epoch": 3.128332740846072,
      "grad_norm": 11.214747428894043,
      "learning_rate": 3.817790417506023e-05,
      "loss": 0.5809,
      "step": 8800
    },
    {
      "epoch": 3.146107358691788,
      "grad_norm": 18.944015502929688,
      "learning_rate": 3.807915629813959e-05,
      "loss": 0.5654,
      "step": 8850
    },
    {
      "epoch": 3.1638819765375046,
      "grad_norm": 26.34320640563965,
      "learning_rate": 3.798040842121894e-05,
      "loss": 0.6507,
      "step": 8900
    },
    {
      "epoch": 3.1816565943832207,
      "grad_norm": 27.523849487304688,
      "learning_rate": 3.78816605442983e-05,
      "loss": 0.5862,
      "step": 8950
    },
    {
      "epoch": 3.1994312122289372,
      "grad_norm": 19.137331008911133,
      "learning_rate": 3.778291266737765e-05,
      "loss": 0.6941,
      "step": 9000
    },
    {
      "epoch": 3.2172058300746533,
      "grad_norm": 38.839847564697266,
      "learning_rate": 3.7684164790457003e-05,
      "loss": 0.588,
      "step": 9050
    },
    {
      "epoch": 3.23498044792037,
      "grad_norm": 24.464548110961914,
      "learning_rate": 3.758541691353636e-05,
      "loss": 0.761,
      "step": 9100
    },
    {
      "epoch": 3.252755065766086,
      "grad_norm": 31.030643463134766,
      "learning_rate": 3.748666903661571e-05,
      "loss": 0.5936,
      "step": 9150
    },
    {
      "epoch": 3.2705296836118025,
      "grad_norm": 13.451333999633789,
      "learning_rate": 3.7387921159695064e-05,
      "loss": 0.7235,
      "step": 9200
    },
    {
      "epoch": 3.2883043014575186,
      "grad_norm": 26.44865608215332,
      "learning_rate": 3.728917328277442e-05,
      "loss": 0.5973,
      "step": 9250
    },
    {
      "epoch": 3.306078919303235,
      "grad_norm": 17.371505737304688,
      "learning_rate": 3.7190425405853774e-05,
      "loss": 0.6346,
      "step": 9300
    },
    {
      "epoch": 3.3238535371489513,
      "grad_norm": 35.123538970947266,
      "learning_rate": 3.7091677528933125e-05,
      "loss": 0.5805,
      "step": 9350
    },
    {
      "epoch": 3.341628154994668,
      "grad_norm": 42.86513900756836,
      "learning_rate": 3.6992929652012483e-05,
      "loss": 0.6318,
      "step": 9400
    },
    {
      "epoch": 3.359402772840384,
      "grad_norm": 23.84062957763672,
      "learning_rate": 3.6894181775091835e-05,
      "loss": 0.5973,
      "step": 9450
    },
    {
      "epoch": 3.3771773906861,
      "grad_norm": 29.349319458007812,
      "learning_rate": 3.6795433898171186e-05,
      "loss": 0.6459,
      "step": 9500
    },
    {
      "epoch": 3.3949520085318166,
      "grad_norm": 19.391407012939453,
      "learning_rate": 3.6696686021250544e-05,
      "loss": 0.632,
      "step": 9550
    },
    {
      "epoch": 3.412726626377533,
      "grad_norm": 24.438249588012695,
      "learning_rate": 3.6597938144329896e-05,
      "loss": 0.7181,
      "step": 9600
    },
    {
      "epoch": 3.430501244223249,
      "grad_norm": 14.284900665283203,
      "learning_rate": 3.649919026740925e-05,
      "loss": 0.6143,
      "step": 9650
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 21.91012191772461,
      "learning_rate": 3.6400442390488605e-05,
      "loss": 0.6533,
      "step": 9700
    },
    {
      "epoch": 3.466050479914682,
      "grad_norm": 26.20189094543457,
      "learning_rate": 3.630169451356796e-05,
      "loss": 0.6168,
      "step": 9750
    },
    {
      "epoch": 3.4838250977603984,
      "grad_norm": 28.79902458190918,
      "learning_rate": 3.620294663664731e-05,
      "loss": 0.6459,
      "step": 9800
    },
    {
      "epoch": 3.5015997156061145,
      "grad_norm": 23.52932357788086,
      "learning_rate": 3.6104198759726666e-05,
      "loss": 0.6944,
      "step": 9850
    },
    {
      "epoch": 3.5193743334518306,
      "grad_norm": 15.69887924194336,
      "learning_rate": 3.600545088280602e-05,
      "loss": 0.6383,
      "step": 9900
    },
    {
      "epoch": 3.537148951297547,
      "grad_norm": 27.866592407226562,
      "learning_rate": 3.5906703005885376e-05,
      "loss": 0.5906,
      "step": 9950
    },
    {
      "epoch": 3.5549235691432637,
      "grad_norm": 22.662351608276367,
      "learning_rate": 3.580795512896473e-05,
      "loss": 0.6757,
      "step": 10000
    },
    {
      "epoch": 3.57269818698898,
      "grad_norm": 23.2382869720459,
      "learning_rate": 3.570920725204408e-05,
      "loss": 0.6874,
      "step": 10050
    },
    {
      "epoch": 3.590472804834696,
      "grad_norm": 32.9306526184082,
      "learning_rate": 3.561045937512344e-05,
      "loss": 0.602,
      "step": 10100
    },
    {
      "epoch": 3.6082474226804124,
      "grad_norm": 20.117294311523438,
      "learning_rate": 3.551171149820279e-05,
      "loss": 0.645,
      "step": 10150
    },
    {
      "epoch": 3.6260220405261285,
      "grad_norm": 19.04990577697754,
      "learning_rate": 3.541296362128214e-05,
      "loss": 0.6918,
      "step": 10200
    },
    {
      "epoch": 3.643796658371845,
      "grad_norm": 30.83713150024414,
      "learning_rate": 3.53142157443615e-05,
      "loss": 0.6288,
      "step": 10250
    },
    {
      "epoch": 3.661571276217561,
      "grad_norm": 29.392410278320312,
      "learning_rate": 3.5215467867440856e-05,
      "loss": 0.546,
      "step": 10300
    },
    {
      "epoch": 3.6793458940632777,
      "grad_norm": 20.53127670288086,
      "learning_rate": 3.511671999052021e-05,
      "loss": 0.6232,
      "step": 10350
    },
    {
      "epoch": 3.697120511908994,
      "grad_norm": 28.859729766845703,
      "learning_rate": 3.501797211359956e-05,
      "loss": 0.6437,
      "step": 10400
    },
    {
      "epoch": 3.7148951297547104,
      "grad_norm": 26.614883422851562,
      "learning_rate": 3.491922423667892e-05,
      "loss": 0.649,
      "step": 10450
    },
    {
      "epoch": 3.7326697476004265,
      "grad_norm": 15.354384422302246,
      "learning_rate": 3.482047635975827e-05,
      "loss": 0.5608,
      "step": 10500
    },
    {
      "epoch": 3.750444365446143,
      "grad_norm": 34.40671157836914,
      "learning_rate": 3.4721728482837626e-05,
      "loss": 0.6338,
      "step": 10550
    },
    {
      "epoch": 3.768218983291859,
      "grad_norm": 33.3489990234375,
      "learning_rate": 3.462298060591698e-05,
      "loss": 0.6203,
      "step": 10600
    },
    {
      "epoch": 3.7859936011375757,
      "grad_norm": 30.411245346069336,
      "learning_rate": 3.452423272899633e-05,
      "loss": 0.6189,
      "step": 10650
    },
    {
      "epoch": 3.8037682189832918,
      "grad_norm": 18.49363899230957,
      "learning_rate": 3.442548485207569e-05,
      "loss": 0.653,
      "step": 10700
    },
    {
      "epoch": 3.8215428368290083,
      "grad_norm": 18.03886604309082,
      "learning_rate": 3.432673697515504e-05,
      "loss": 0.6549,
      "step": 10750
    },
    {
      "epoch": 3.8393174546747244,
      "grad_norm": 32.8499641418457,
      "learning_rate": 3.422798909823439e-05,
      "loss": 0.6347,
      "step": 10800
    },
    {
      "epoch": 3.857092072520441,
      "grad_norm": 21.178979873657227,
      "learning_rate": 3.412924122131375e-05,
      "loss": 0.6284,
      "step": 10850
    },
    {
      "epoch": 3.874866690366157,
      "grad_norm": 19.413715362548828,
      "learning_rate": 3.40304933443931e-05,
      "loss": 0.6348,
      "step": 10900
    },
    {
      "epoch": 3.8926413082118736,
      "grad_norm": 25.282211303710938,
      "learning_rate": 3.393174546747245e-05,
      "loss": 0.5854,
      "step": 10950
    },
    {
      "epoch": 3.9104159260575897,
      "grad_norm": 17.415884017944336,
      "learning_rate": 3.383299759055181e-05,
      "loss": 0.6568,
      "step": 11000
    },
    {
      "epoch": 3.9281905439033062,
      "grad_norm": 18.71302604675293,
      "learning_rate": 3.373424971363116e-05,
      "loss": 0.7064,
      "step": 11050
    },
    {
      "epoch": 3.9459651617490223,
      "grad_norm": 23.446887969970703,
      "learning_rate": 3.363550183671051e-05,
      "loss": 0.5959,
      "step": 11100
    },
    {
      "epoch": 3.9637397795947384,
      "grad_norm": 35.751251220703125,
      "learning_rate": 3.353675395978987e-05,
      "loss": 0.6836,
      "step": 11150
    },
    {
      "epoch": 3.981514397440455,
      "grad_norm": 21.46914291381836,
      "learning_rate": 3.343800608286922e-05,
      "loss": 0.6329,
      "step": 11200
    },
    {
      "epoch": 3.9992890152861715,
      "grad_norm": 20.236717224121094,
      "learning_rate": 3.333925820594857e-05,
      "loss": 0.6809,
      "step": 11250
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.4252818822860718,
      "eval_runtime": 612.7145,
      "eval_samples_per_second": 8.16,
      "eval_steps_per_second": 0.511,
      "step": 11252
    },
    {
      "epoch": 4.017063633131888,
      "grad_norm": 26.28738784790039,
      "learning_rate": 3.324051032902793e-05,
      "loss": 0.3489,
      "step": 11300
    },
    {
      "epoch": 4.034838250977604,
      "grad_norm": 5.1397809982299805,
      "learning_rate": 3.314176245210728e-05,
      "loss": 0.3019,
      "step": 11350
    },
    {
      "epoch": 4.052612868823321,
      "grad_norm": 16.36956214904785,
      "learning_rate": 3.3043014575186634e-05,
      "loss": 0.2902,
      "step": 11400
    },
    {
      "epoch": 4.070387486669037,
      "grad_norm": 20.106477737426758,
      "learning_rate": 3.294426669826599e-05,
      "loss": 0.3157,
      "step": 11450
    },
    {
      "epoch": 4.088162104514753,
      "grad_norm": 44.09709930419922,
      "learning_rate": 3.2845518821345343e-05,
      "loss": 0.3435,
      "step": 11500
    },
    {
      "epoch": 4.105936722360469,
      "grad_norm": 20.194002151489258,
      "learning_rate": 3.27467709444247e-05,
      "loss": 0.3236,
      "step": 11550
    },
    {
      "epoch": 4.123711340206185,
      "grad_norm": 30.338281631469727,
      "learning_rate": 3.264802306750405e-05,
      "loss": 0.3514,
      "step": 11600
    },
    {
      "epoch": 4.141485958051902,
      "grad_norm": 22.332063674926758,
      "learning_rate": 3.2549275190583404e-05,
      "loss": 0.3573,
      "step": 11650
    },
    {
      "epoch": 4.159260575897618,
      "grad_norm": 28.593414306640625,
      "learning_rate": 3.245052731366276e-05,
      "loss": 0.3544,
      "step": 11700
    },
    {
      "epoch": 4.177035193743334,
      "grad_norm": 33.09299850463867,
      "learning_rate": 3.2351779436742114e-05,
      "loss": 0.289,
      "step": 11750
    },
    {
      "epoch": 4.19480981158905,
      "grad_norm": 7.052907943725586,
      "learning_rate": 3.2253031559821465e-05,
      "loss": 0.3145,
      "step": 11800
    },
    {
      "epoch": 4.212584429434767,
      "grad_norm": 15.328741073608398,
      "learning_rate": 3.2154283682900823e-05,
      "loss": 0.3659,
      "step": 11850
    },
    {
      "epoch": 4.2303590472804835,
      "grad_norm": 35.098175048828125,
      "learning_rate": 3.2055535805980175e-05,
      "loss": 0.2856,
      "step": 11900
    },
    {
      "epoch": 4.2481336651262,
      "grad_norm": 24.490201950073242,
      "learning_rate": 3.1956787929059526e-05,
      "loss": 0.2929,
      "step": 11950
    },
    {
      "epoch": 4.265908282971916,
      "grad_norm": 14.10155963897705,
      "learning_rate": 3.1858040052138884e-05,
      "loss": 0.3125,
      "step": 12000
    },
    {
      "epoch": 4.283682900817633,
      "grad_norm": 28.332578659057617,
      "learning_rate": 3.1759292175218236e-05,
      "loss": 0.3217,
      "step": 12050
    },
    {
      "epoch": 4.301457518663349,
      "grad_norm": 19.941715240478516,
      "learning_rate": 3.166054429829759e-05,
      "loss": 0.3503,
      "step": 12100
    },
    {
      "epoch": 4.319232136509065,
      "grad_norm": 31.504701614379883,
      "learning_rate": 3.1561796421376945e-05,
      "loss": 0.3461,
      "step": 12150
    },
    {
      "epoch": 4.337006754354781,
      "grad_norm": 9.094016075134277,
      "learning_rate": 3.14630485444563e-05,
      "loss": 0.3116,
      "step": 12200
    },
    {
      "epoch": 4.354781372200498,
      "grad_norm": 28.628734588623047,
      "learning_rate": 3.136430066753565e-05,
      "loss": 0.2764,
      "step": 12250
    },
    {
      "epoch": 4.372555990046214,
      "grad_norm": 23.97233772277832,
      "learning_rate": 3.1265552790615006e-05,
      "loss": 0.3863,
      "step": 12300
    },
    {
      "epoch": 4.39033060789193,
      "grad_norm": 13.69547176361084,
      "learning_rate": 3.116680491369436e-05,
      "loss": 0.3379,
      "step": 12350
    },
    {
      "epoch": 4.408105225737646,
      "grad_norm": 22.47005271911621,
      "learning_rate": 3.106805703677371e-05,
      "loss": 0.2832,
      "step": 12400
    },
    {
      "epoch": 4.425879843583363,
      "grad_norm": 29.116649627685547,
      "learning_rate": 3.096930915985307e-05,
      "loss": 0.3273,
      "step": 12450
    },
    {
      "epoch": 4.443654461429079,
      "grad_norm": 27.6177978515625,
      "learning_rate": 3.087056128293242e-05,
      "loss": 0.378,
      "step": 12500
    },
    {
      "epoch": 4.4614290792747955,
      "grad_norm": 33.54322052001953,
      "learning_rate": 3.077181340601178e-05,
      "loss": 0.3566,
      "step": 12550
    },
    {
      "epoch": 4.479203697120512,
      "grad_norm": 13.55495548248291,
      "learning_rate": 3.067306552909113e-05,
      "loss": 0.38,
      "step": 12600
    },
    {
      "epoch": 4.496978314966229,
      "grad_norm": 10.155524253845215,
      "learning_rate": 3.057431765217048e-05,
      "loss": 0.3664,
      "step": 12650
    },
    {
      "epoch": 4.514752932811945,
      "grad_norm": 34.01736831665039,
      "learning_rate": 3.0475569775249834e-05,
      "loss": 0.3195,
      "step": 12700
    },
    {
      "epoch": 4.532527550657661,
      "grad_norm": 6.52284574508667,
      "learning_rate": 3.037682189832919e-05,
      "loss": 0.3033,
      "step": 12750
    },
    {
      "epoch": 4.550302168503377,
      "grad_norm": 25.09425926208496,
      "learning_rate": 3.0278074021408544e-05,
      "loss": 0.4003,
      "step": 12800
    },
    {
      "epoch": 4.568076786349094,
      "grad_norm": 20.682859420776367,
      "learning_rate": 3.0179326144487895e-05,
      "loss": 0.3694,
      "step": 12850
    },
    {
      "epoch": 4.58585140419481,
      "grad_norm": 9.262455940246582,
      "learning_rate": 3.008057826756725e-05,
      "loss": 0.3388,
      "step": 12900
    },
    {
      "epoch": 4.603626022040526,
      "grad_norm": 18.448575973510742,
      "learning_rate": 2.9981830390646605e-05,
      "loss": 0.3431,
      "step": 12950
    },
    {
      "epoch": 4.621400639886242,
      "grad_norm": 29.990999221801758,
      "learning_rate": 2.9883082513725956e-05,
      "loss": 0.3937,
      "step": 13000
    },
    {
      "epoch": 4.639175257731958,
      "grad_norm": 36.05508804321289,
      "learning_rate": 2.978433463680531e-05,
      "loss": 0.3453,
      "step": 13050
    },
    {
      "epoch": 4.656949875577675,
      "grad_norm": 16.277252197265625,
      "learning_rate": 2.9685586759884666e-05,
      "loss": 0.3493,
      "step": 13100
    },
    {
      "epoch": 4.674724493423391,
      "grad_norm": 20.95882797241211,
      "learning_rate": 2.9586838882964017e-05,
      "loss": 0.3393,
      "step": 13150
    },
    {
      "epoch": 4.6924991112691075,
      "grad_norm": 10.493337631225586,
      "learning_rate": 2.9488091006043372e-05,
      "loss": 0.3629,
      "step": 13200
    },
    {
      "epoch": 4.7102737291148244,
      "grad_norm": 18.928951263427734,
      "learning_rate": 2.9389343129122727e-05,
      "loss": 0.3347,
      "step": 13250
    },
    {
      "epoch": 4.7280483469605405,
      "grad_norm": 8.087231636047363,
      "learning_rate": 2.929059525220208e-05,
      "loss": 0.3495,
      "step": 13300
    },
    {
      "epoch": 4.745822964806257,
      "grad_norm": 21.5911865234375,
      "learning_rate": 2.9191847375281433e-05,
      "loss": 0.3206,
      "step": 13350
    },
    {
      "epoch": 4.763597582651973,
      "grad_norm": 18.53110694885254,
      "learning_rate": 2.9093099498360788e-05,
      "loss": 0.3464,
      "step": 13400
    },
    {
      "epoch": 4.781372200497689,
      "grad_norm": 43.2369384765625,
      "learning_rate": 2.8994351621440142e-05,
      "loss": 0.371,
      "step": 13450
    },
    {
      "epoch": 4.799146818343406,
      "grad_norm": 20.332012176513672,
      "learning_rate": 2.8895603744519494e-05,
      "loss": 0.3741,
      "step": 13500
    },
    {
      "epoch": 4.816921436189122,
      "grad_norm": 25.648473739624023,
      "learning_rate": 2.879685586759885e-05,
      "loss": 0.3153,
      "step": 13550
    },
    {
      "epoch": 4.834696054034838,
      "grad_norm": 27.965686798095703,
      "learning_rate": 2.8698107990678203e-05,
      "loss": 0.372,
      "step": 13600
    },
    {
      "epoch": 4.852470671880555,
      "grad_norm": 25.140275955200195,
      "learning_rate": 2.8599360113757555e-05,
      "loss": 0.3491,
      "step": 13650
    },
    {
      "epoch": 4.870245289726271,
      "grad_norm": 16.192298889160156,
      "learning_rate": 2.850061223683691e-05,
      "loss": 0.3339,
      "step": 13700
    },
    {
      "epoch": 4.888019907571987,
      "grad_norm": 14.041748046875,
      "learning_rate": 2.8401864359916264e-05,
      "loss": 0.2957,
      "step": 13750
    },
    {
      "epoch": 4.905794525417703,
      "grad_norm": 18.597122192382812,
      "learning_rate": 2.830311648299562e-05,
      "loss": 0.3395,
      "step": 13800
    },
    {
      "epoch": 4.923569143263419,
      "grad_norm": 18.48052215576172,
      "learning_rate": 2.820436860607497e-05,
      "loss": 0.3563,
      "step": 13850
    },
    {
      "epoch": 4.941343761109136,
      "grad_norm": 32.82701110839844,
      "learning_rate": 2.8105620729154325e-05,
      "loss": 0.3499,
      "step": 13900
    },
    {
      "epoch": 4.9591183789548525,
      "grad_norm": 9.885085105895996,
      "learning_rate": 2.800687285223368e-05,
      "loss": 0.3601,
      "step": 13950
    },
    {
      "epoch": 4.976892996800569,
      "grad_norm": 20.4886474609375,
      "learning_rate": 2.790812497531303e-05,
      "loss": 0.352,
      "step": 14000
    },
    {
      "epoch": 4.994667614646285,
      "grad_norm": 23.855506896972656,
      "learning_rate": 2.7809377098392386e-05,
      "loss": 0.3583,
      "step": 14050
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.471807837486267,
      "eval_runtime": 613.0327,
      "eval_samples_per_second": 8.156,
      "eval_steps_per_second": 0.511,
      "step": 14065
    },
    {
      "epoch": 5.012442232492002,
      "grad_norm": 9.841597557067871,
      "learning_rate": 2.771062922147174e-05,
      "loss": 0.237,
      "step": 14100
    },
    {
      "epoch": 5.030216850337718,
      "grad_norm": 35.218807220458984,
      "learning_rate": 2.7611881344551092e-05,
      "loss": 0.1301,
      "step": 14150
    },
    {
      "epoch": 5.047991468183434,
      "grad_norm": 22.174524307250977,
      "learning_rate": 2.7513133467630447e-05,
      "loss": 0.1552,
      "step": 14200
    },
    {
      "epoch": 5.06576608602915,
      "grad_norm": 2.129495620727539,
      "learning_rate": 2.7414385590709802e-05,
      "loss": 0.123,
      "step": 14250
    },
    {
      "epoch": 5.083540703874867,
      "grad_norm": 9.539194107055664,
      "learning_rate": 2.7315637713789157e-05,
      "loss": 0.1313,
      "step": 14300
    },
    {
      "epoch": 5.101315321720583,
      "grad_norm": 16.209861755371094,
      "learning_rate": 2.7216889836868508e-05,
      "loss": 0.1495,
      "step": 14350
    },
    {
      "epoch": 5.119089939566299,
      "grad_norm": 13.791181564331055,
      "learning_rate": 2.7118141959947863e-05,
      "loss": 0.1569,
      "step": 14400
    },
    {
      "epoch": 5.136864557412015,
      "grad_norm": 31.02383804321289,
      "learning_rate": 2.7019394083027218e-05,
      "loss": 0.1311,
      "step": 14450
    },
    {
      "epoch": 5.154639175257732,
      "grad_norm": 8.873542785644531,
      "learning_rate": 2.692064620610657e-05,
      "loss": 0.1459,
      "step": 14500
    },
    {
      "epoch": 5.172413793103448,
      "grad_norm": 12.069522857666016,
      "learning_rate": 2.6821898329185924e-05,
      "loss": 0.1303,
      "step": 14550
    },
    {
      "epoch": 5.1901884109491645,
      "grad_norm": 3.795351505279541,
      "learning_rate": 2.672315045226528e-05,
      "loss": 0.1964,
      "step": 14600
    },
    {
      "epoch": 5.207963028794881,
      "grad_norm": 7.288588047027588,
      "learning_rate": 2.662440257534463e-05,
      "loss": 0.1941,
      "step": 14650
    },
    {
      "epoch": 5.225737646640598,
      "grad_norm": 32.9359245300293,
      "learning_rate": 2.6525654698423985e-05,
      "loss": 0.1506,
      "step": 14700
    },
    {
      "epoch": 5.243512264486314,
      "grad_norm": 42.36709213256836,
      "learning_rate": 2.642690682150334e-05,
      "loss": 0.1736,
      "step": 14750
    },
    {
      "epoch": 5.26128688233203,
      "grad_norm": 22.95355224609375,
      "learning_rate": 2.6328158944582694e-05,
      "loss": 0.138,
      "step": 14800
    },
    {
      "epoch": 5.279061500177746,
      "grad_norm": 27.840890884399414,
      "learning_rate": 2.6229411067662046e-05,
      "loss": 0.1528,
      "step": 14850
    },
    {
      "epoch": 5.296836118023463,
      "grad_norm": 8.980558395385742,
      "learning_rate": 2.61306631907414e-05,
      "loss": 0.1676,
      "step": 14900
    },
    {
      "epoch": 5.314610735869179,
      "grad_norm": 6.917555809020996,
      "learning_rate": 2.6031915313820755e-05,
      "loss": 0.1756,
      "step": 14950
    },
    {
      "epoch": 5.332385353714895,
      "grad_norm": 1.1334984302520752,
      "learning_rate": 2.5933167436900107e-05,
      "loss": 0.1503,
      "step": 15000
    },
    {
      "epoch": 5.350159971560611,
      "grad_norm": 9.347962379455566,
      "learning_rate": 2.583441955997946e-05,
      "loss": 0.1541,
      "step": 15050
    },
    {
      "epoch": 5.367934589406328,
      "grad_norm": 27.36943244934082,
      "learning_rate": 2.5735671683058816e-05,
      "loss": 0.1669,
      "step": 15100
    },
    {
      "epoch": 5.385709207252044,
      "grad_norm": 14.649333953857422,
      "learning_rate": 2.5636923806138167e-05,
      "loss": 0.1606,
      "step": 15150
    },
    {
      "epoch": 5.40348382509776,
      "grad_norm": 14.340384483337402,
      "learning_rate": 2.5538175929217522e-05,
      "loss": 0.1729,
      "step": 15200
    },
    {
      "epoch": 5.4212584429434765,
      "grad_norm": 33.339752197265625,
      "learning_rate": 2.5439428052296877e-05,
      "loss": 0.1906,
      "step": 15250
    },
    {
      "epoch": 5.439033060789193,
      "grad_norm": 15.357388496398926,
      "learning_rate": 2.5340680175376232e-05,
      "loss": 0.1643,
      "step": 15300
    },
    {
      "epoch": 5.45680767863491,
      "grad_norm": 8.292274475097656,
      "learning_rate": 2.5241932298455583e-05,
      "loss": 0.1749,
      "step": 15350
    },
    {
      "epoch": 5.474582296480626,
      "grad_norm": 1.0181046724319458,
      "learning_rate": 2.5143184421534938e-05,
      "loss": 0.1285,
      "step": 15400
    },
    {
      "epoch": 5.492356914326342,
      "grad_norm": 15.740450859069824,
      "learning_rate": 2.5044436544614293e-05,
      "loss": 0.1559,
      "step": 15450
    },
    {
      "epoch": 5.510131532172059,
      "grad_norm": 14.779481887817383,
      "learning_rate": 2.4945688667693644e-05,
      "loss": 0.2074,
      "step": 15500
    },
    {
      "epoch": 5.527906150017775,
      "grad_norm": 9.954672813415527,
      "learning_rate": 2.4846940790773e-05,
      "loss": 0.1969,
      "step": 15550
    },
    {
      "epoch": 5.545680767863491,
      "grad_norm": 26.68191146850586,
      "learning_rate": 2.4748192913852354e-05,
      "loss": 0.1875,
      "step": 15600
    },
    {
      "epoch": 5.563455385709207,
      "grad_norm": 33.072227478027344,
      "learning_rate": 2.4649445036931705e-05,
      "loss": 0.1856,
      "step": 15650
    },
    {
      "epoch": 5.581230003554923,
      "grad_norm": 20.54491424560547,
      "learning_rate": 2.455069716001106e-05,
      "loss": 0.1527,
      "step": 15700
    },
    {
      "epoch": 5.59900462140064,
      "grad_norm": 20.19522476196289,
      "learning_rate": 2.4451949283090415e-05,
      "loss": 0.1486,
      "step": 15750
    },
    {
      "epoch": 5.616779239246356,
      "grad_norm": 12.979328155517578,
      "learning_rate": 2.435320140616977e-05,
      "loss": 0.1826,
      "step": 15800
    },
    {
      "epoch": 5.634553857092072,
      "grad_norm": 13.95683765411377,
      "learning_rate": 2.425445352924912e-05,
      "loss": 0.1757,
      "step": 15850
    },
    {
      "epoch": 5.6523284749377884,
      "grad_norm": 27.335067749023438,
      "learning_rate": 2.4155705652328476e-05,
      "loss": 0.1631,
      "step": 15900
    },
    {
      "epoch": 5.670103092783505,
      "grad_norm": 33.39929962158203,
      "learning_rate": 2.405695777540783e-05,
      "loss": 0.1507,
      "step": 15950
    },
    {
      "epoch": 5.6878777106292215,
      "grad_norm": 25.400089263916016,
      "learning_rate": 2.3958209898487182e-05,
      "loss": 0.1578,
      "step": 16000
    },
    {
      "epoch": 5.705652328474938,
      "grad_norm": 38.8485107421875,
      "learning_rate": 2.3859462021566536e-05,
      "loss": 0.1459,
      "step": 16050
    },
    {
      "epoch": 5.723426946320654,
      "grad_norm": 2.9796719551086426,
      "learning_rate": 2.376071414464589e-05,
      "loss": 0.1881,
      "step": 16100
    },
    {
      "epoch": 5.741201564166371,
      "grad_norm": 53.08714294433594,
      "learning_rate": 2.3661966267725243e-05,
      "loss": 0.1838,
      "step": 16150
    },
    {
      "epoch": 5.758976182012087,
      "grad_norm": 32.09259796142578,
      "learning_rate": 2.3563218390804597e-05,
      "loss": 0.1786,
      "step": 16200
    },
    {
      "epoch": 5.776750799857803,
      "grad_norm": 19.822481155395508,
      "learning_rate": 2.3464470513883952e-05,
      "loss": 0.1639,
      "step": 16250
    },
    {
      "epoch": 5.794525417703519,
      "grad_norm": 35.06816864013672,
      "learning_rate": 2.3365722636963307e-05,
      "loss": 0.1911,
      "step": 16300
    },
    {
      "epoch": 5.812300035549236,
      "grad_norm": 16.73479461669922,
      "learning_rate": 2.326697476004266e-05,
      "loss": 0.1678,
      "step": 16350
    },
    {
      "epoch": 5.830074653394952,
      "grad_norm": 10.01861572265625,
      "learning_rate": 2.3168226883122013e-05,
      "loss": 0.1541,
      "step": 16400
    },
    {
      "epoch": 5.847849271240668,
      "grad_norm": 2.636895179748535,
      "learning_rate": 2.3069479006201368e-05,
      "loss": 0.1877,
      "step": 16450
    },
    {
      "epoch": 5.865623889086384,
      "grad_norm": 25.35785484313965,
      "learning_rate": 2.297073112928072e-05,
      "loss": 0.156,
      "step": 16500
    },
    {
      "epoch": 5.883398506932101,
      "grad_norm": 23.089689254760742,
      "learning_rate": 2.2871983252360074e-05,
      "loss": 0.1963,
      "step": 16550
    },
    {
      "epoch": 5.901173124777817,
      "grad_norm": 6.032571792602539,
      "learning_rate": 2.277323537543943e-05,
      "loss": 0.1868,
      "step": 16600
    },
    {
      "epoch": 5.9189477426235335,
      "grad_norm": 39.39674377441406,
      "learning_rate": 2.267448749851878e-05,
      "loss": 0.173,
      "step": 16650
    },
    {
      "epoch": 5.93672236046925,
      "grad_norm": 19.42482566833496,
      "learning_rate": 2.2575739621598135e-05,
      "loss": 0.1643,
      "step": 16700
    },
    {
      "epoch": 5.954496978314967,
      "grad_norm": 2.7887542247772217,
      "learning_rate": 2.247699174467749e-05,
      "loss": 0.116,
      "step": 16750
    },
    {
      "epoch": 5.972271596160683,
      "grad_norm": 1.013600468635559,
      "learning_rate": 2.2378243867756845e-05,
      "loss": 0.1512,
      "step": 16800
    },
    {
      "epoch": 5.990046214006399,
      "grad_norm": 23.426345825195312,
      "learning_rate": 2.22794959908362e-05,
      "loss": 0.146,
      "step": 16850
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.7688312530517578,
      "eval_runtime": 602.5985,
      "eval_samples_per_second": 8.297,
      "eval_steps_per_second": 0.519,
      "step": 16878
    },
    {
      "epoch": 6.007820831852115,
      "grad_norm": 1.0317221879959106,
      "learning_rate": 2.2180748113915554e-05,
      "loss": 0.0997,
      "step": 16900
    },
    {
      "epoch": 6.025595449697832,
      "grad_norm": 3.0780818462371826,
      "learning_rate": 2.2082000236994906e-05,
      "loss": 0.0524,
      "step": 16950
    },
    {
      "epoch": 6.043370067543548,
      "grad_norm": 22.159366607666016,
      "learning_rate": 2.198325236007426e-05,
      "loss": 0.0774,
      "step": 17000
    },
    {
      "epoch": 6.061144685389264,
      "grad_norm": 25.09166717529297,
      "learning_rate": 2.1884504483153615e-05,
      "loss": 0.0744,
      "step": 17050
    },
    {
      "epoch": 6.07891930323498,
      "grad_norm": 4.3954758644104,
      "learning_rate": 2.178575660623297e-05,
      "loss": 0.0661,
      "step": 17100
    },
    {
      "epoch": 6.096693921080697,
      "grad_norm": 23.98712921142578,
      "learning_rate": 2.168700872931232e-05,
      "loss": 0.0712,
      "step": 17150
    },
    {
      "epoch": 6.114468538926413,
      "grad_norm": 8.105607986450195,
      "learning_rate": 2.1588260852391676e-05,
      "loss": 0.0419,
      "step": 17200
    },
    {
      "epoch": 6.132243156772129,
      "grad_norm": 4.8319010734558105,
      "learning_rate": 2.148951297547103e-05,
      "loss": 0.0819,
      "step": 17250
    },
    {
      "epoch": 6.1500177746178455,
      "grad_norm": 13.757258415222168,
      "learning_rate": 2.1390765098550382e-05,
      "loss": 0.0775,
      "step": 17300
    },
    {
      "epoch": 6.1677923924635625,
      "grad_norm": 18.439260482788086,
      "learning_rate": 2.1292017221629737e-05,
      "loss": 0.0561,
      "step": 17350
    },
    {
      "epoch": 6.185567010309279,
      "grad_norm": 37.670658111572266,
      "learning_rate": 2.1193269344709092e-05,
      "loss": 0.0901,
      "step": 17400
    },
    {
      "epoch": 6.203341628154995,
      "grad_norm": 26.686145782470703,
      "learning_rate": 2.1094521467788443e-05,
      "loss": 0.0639,
      "step": 17450
    },
    {
      "epoch": 6.221116246000711,
      "grad_norm": 0.33421021699905396,
      "learning_rate": 2.0995773590867798e-05,
      "loss": 0.0695,
      "step": 17500
    },
    {
      "epoch": 6.238890863846427,
      "grad_norm": 0.19376535713672638,
      "learning_rate": 2.0897025713947153e-05,
      "loss": 0.0771,
      "step": 17550
    },
    {
      "epoch": 6.256665481692144,
      "grad_norm": 9.354327201843262,
      "learning_rate": 2.0798277837026507e-05,
      "loss": 0.0891,
      "step": 17600
    },
    {
      "epoch": 6.27444009953786,
      "grad_norm": 0.5451663136482239,
      "learning_rate": 2.069952996010586e-05,
      "loss": 0.0639,
      "step": 17650
    },
    {
      "epoch": 6.292214717383576,
      "grad_norm": 15.060952186584473,
      "learning_rate": 2.0600782083185214e-05,
      "loss": 0.0632,
      "step": 17700
    },
    {
      "epoch": 6.309989335229292,
      "grad_norm": 1.5782026052474976,
      "learning_rate": 2.050203420626457e-05,
      "loss": 0.0943,
      "step": 17750
    },
    {
      "epoch": 6.327763953075009,
      "grad_norm": 8.910181045532227,
      "learning_rate": 2.040328632934392e-05,
      "loss": 0.0829,
      "step": 17800
    },
    {
      "epoch": 6.345538570920725,
      "grad_norm": 4.257717132568359,
      "learning_rate": 2.0304538452423275e-05,
      "loss": 0.0624,
      "step": 17850
    },
    {
      "epoch": 6.363313188766441,
      "grad_norm": 22.265657424926758,
      "learning_rate": 2.020579057550263e-05,
      "loss": 0.0572,
      "step": 17900
    },
    {
      "epoch": 6.3810878066121575,
      "grad_norm": 23.71601676940918,
      "learning_rate": 2.010704269858198e-05,
      "loss": 0.0758,
      "step": 17950
    },
    {
      "epoch": 6.3988624244578745,
      "grad_norm": 44.459739685058594,
      "learning_rate": 2.0008294821661335e-05,
      "loss": 0.082,
      "step": 18000
    },
    {
      "epoch": 6.4166370423035906,
      "grad_norm": 18.776216506958008,
      "learning_rate": 1.990954694474069e-05,
      "loss": 0.0632,
      "step": 18050
    },
    {
      "epoch": 6.434411660149307,
      "grad_norm": 0.027380552142858505,
      "learning_rate": 1.9810799067820045e-05,
      "loss": 0.0606,
      "step": 18100
    },
    {
      "epoch": 6.452186277995023,
      "grad_norm": 3.9534823894500732,
      "learning_rate": 1.9712051190899396e-05,
      "loss": 0.0832,
      "step": 18150
    },
    {
      "epoch": 6.46996089584074,
      "grad_norm": 39.3613395690918,
      "learning_rate": 1.961330331397875e-05,
      "loss": 0.0636,
      "step": 18200
    },
    {
      "epoch": 6.487735513686456,
      "grad_norm": 0.5277191400527954,
      "learning_rate": 1.9514555437058106e-05,
      "loss": 0.1124,
      "step": 18250
    },
    {
      "epoch": 6.505510131532172,
      "grad_norm": 0.7152730822563171,
      "learning_rate": 1.9415807560137457e-05,
      "loss": 0.0581,
      "step": 18300
    },
    {
      "epoch": 6.523284749377888,
      "grad_norm": 2.3581998348236084,
      "learning_rate": 1.9317059683216812e-05,
      "loss": 0.0639,
      "step": 18350
    },
    {
      "epoch": 6.541059367223605,
      "grad_norm": 11.58313274383545,
      "learning_rate": 1.9218311806296167e-05,
      "loss": 0.0495,
      "step": 18400
    },
    {
      "epoch": 6.558833985069321,
      "grad_norm": 18.373676300048828,
      "learning_rate": 1.9119563929375518e-05,
      "loss": 0.0693,
      "step": 18450
    },
    {
      "epoch": 6.576608602915037,
      "grad_norm": 67.02750396728516,
      "learning_rate": 1.9020816052454873e-05,
      "loss": 0.0637,
      "step": 18500
    },
    {
      "epoch": 6.594383220760753,
      "grad_norm": 1.4584473371505737,
      "learning_rate": 1.8922068175534228e-05,
      "loss": 0.0774,
      "step": 18550
    },
    {
      "epoch": 6.61215783860647,
      "grad_norm": 5.059803009033203,
      "learning_rate": 1.8823320298613583e-05,
      "loss": 0.0881,
      "step": 18600
    },
    {
      "epoch": 6.629932456452186,
      "grad_norm": 2.585195302963257,
      "learning_rate": 1.8724572421692934e-05,
      "loss": 0.0646,
      "step": 18650
    },
    {
      "epoch": 6.6477070742979025,
      "grad_norm": 2.248694658279419,
      "learning_rate": 1.862582454477229e-05,
      "loss": 0.0611,
      "step": 18700
    },
    {
      "epoch": 6.665481692143619,
      "grad_norm": 1.8752521276474,
      "learning_rate": 1.8527076667851644e-05,
      "loss": 0.0577,
      "step": 18750
    },
    {
      "epoch": 6.683256309989336,
      "grad_norm": 1.9117751121520996,
      "learning_rate": 1.8428328790930995e-05,
      "loss": 0.0458,
      "step": 18800
    },
    {
      "epoch": 6.701030927835052,
      "grad_norm": 10.677266120910645,
      "learning_rate": 1.832958091401035e-05,
      "loss": 0.0564,
      "step": 18850
    },
    {
      "epoch": 6.718805545680768,
      "grad_norm": 27.287797927856445,
      "learning_rate": 1.8230833037089704e-05,
      "loss": 0.056,
      "step": 18900
    },
    {
      "epoch": 6.736580163526484,
      "grad_norm": 36.80601501464844,
      "learning_rate": 1.8132085160169056e-05,
      "loss": 0.0792,
      "step": 18950
    },
    {
      "epoch": 6.7543547813722,
      "grad_norm": 8.451902389526367,
      "learning_rate": 1.803333728324841e-05,
      "loss": 0.061,
      "step": 19000
    },
    {
      "epoch": 6.772129399217917,
      "grad_norm": 0.8356664776802063,
      "learning_rate": 1.7934589406327765e-05,
      "loss": 0.0448,
      "step": 19050
    },
    {
      "epoch": 6.789904017063633,
      "grad_norm": 4.972860336303711,
      "learning_rate": 1.7835841529407117e-05,
      "loss": 0.0421,
      "step": 19100
    },
    {
      "epoch": 6.807678634909349,
      "grad_norm": 0.1136043518781662,
      "learning_rate": 1.773709365248647e-05,
      "loss": 0.0568,
      "step": 19150
    },
    {
      "epoch": 6.825453252755066,
      "grad_norm": 0.3388063907623291,
      "learning_rate": 1.7638345775565826e-05,
      "loss": 0.075,
      "step": 19200
    },
    {
      "epoch": 6.843227870600782,
      "grad_norm": 3.210336923599243,
      "learning_rate": 1.753959789864518e-05,
      "loss": 0.0599,
      "step": 19250
    },
    {
      "epoch": 6.861002488446498,
      "grad_norm": 13.942416191101074,
      "learning_rate": 1.7440850021724533e-05,
      "loss": 0.0579,
      "step": 19300
    },
    {
      "epoch": 6.8787771062922145,
      "grad_norm": 43.646812438964844,
      "learning_rate": 1.7342102144803887e-05,
      "loss": 0.058,
      "step": 19350
    },
    {
      "epoch": 6.896551724137931,
      "grad_norm": 0.2902136445045471,
      "learning_rate": 1.7243354267883242e-05,
      "loss": 0.0711,
      "step": 19400
    },
    {
      "epoch": 6.914326341983648,
      "grad_norm": 4.602980613708496,
      "learning_rate": 1.7144606390962593e-05,
      "loss": 0.0837,
      "step": 19450
    },
    {
      "epoch": 6.932100959829364,
      "grad_norm": 17.18175506591797,
      "learning_rate": 1.7045858514041948e-05,
      "loss": 0.0918,
      "step": 19500
    },
    {
      "epoch": 6.94987557767508,
      "grad_norm": 1.5822950601577759,
      "learning_rate": 1.6947110637121303e-05,
      "loss": 0.0587,
      "step": 19550
    },
    {
      "epoch": 6.967650195520797,
      "grad_norm": 1.8463311195373535,
      "learning_rate": 1.6848362760200654e-05,
      "loss": 0.079,
      "step": 19600
    },
    {
      "epoch": 6.985424813366513,
      "grad_norm": 1.6112418174743652,
      "learning_rate": 1.674961488328001e-05,
      "loss": 0.0476,
      "step": 19650
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.9813929796218872,
      "eval_runtime": 609.8837,
      "eval_samples_per_second": 8.198,
      "eval_steps_per_second": 0.513,
      "step": 19691
    },
    {
      "epoch": 7.003199431212229,
      "grad_norm": 28.19272804260254,
      "learning_rate": 1.6650867006359364e-05,
      "loss": 0.0704,
      "step": 19700
    },
    {
      "epoch": 7.020974049057945,
      "grad_norm": 0.11712545156478882,
      "learning_rate": 1.655211912943872e-05,
      "loss": 0.0254,
      "step": 19750
    },
    {
      "epoch": 7.038748666903661,
      "grad_norm": 5.567170143127441,
      "learning_rate": 1.645337125251807e-05,
      "loss": 0.0225,
      "step": 19800
    },
    {
      "epoch": 7.056523284749378,
      "grad_norm": 35.201271057128906,
      "learning_rate": 1.6354623375597425e-05,
      "loss": 0.0275,
      "step": 19850
    },
    {
      "epoch": 7.074297902595094,
      "grad_norm": 0.29468095302581787,
      "learning_rate": 1.625587549867678e-05,
      "loss": 0.021,
      "step": 19900
    },
    {
      "epoch": 7.09207252044081,
      "grad_norm": 1.2396035194396973,
      "learning_rate": 1.615712762175613e-05,
      "loss": 0.0283,
      "step": 19950
    },
    {
      "epoch": 7.1098471382865265,
      "grad_norm": 4.146747589111328,
      "learning_rate": 1.6058379744835486e-05,
      "loss": 0.0208,
      "step": 20000
    },
    {
      "epoch": 7.1276217561322435,
      "grad_norm": 12.047792434692383,
      "learning_rate": 1.595963186791484e-05,
      "loss": 0.0227,
      "step": 20050
    },
    {
      "epoch": 7.14539637397796,
      "grad_norm": 6.674045562744141,
      "learning_rate": 1.5860883990994192e-05,
      "loss": 0.0203,
      "step": 20100
    },
    {
      "epoch": 7.163170991823676,
      "grad_norm": 0.10758262127637863,
      "learning_rate": 1.5762136114073547e-05,
      "loss": 0.0166,
      "step": 20150
    },
    {
      "epoch": 7.180945609669392,
      "grad_norm": 3.059586763381958,
      "learning_rate": 1.56633882371529e-05,
      "loss": 0.0327,
      "step": 20200
    },
    {
      "epoch": 7.198720227515109,
      "grad_norm": 51.597511291503906,
      "learning_rate": 1.5564640360232256e-05,
      "loss": 0.0136,
      "step": 20250
    },
    {
      "epoch": 7.216494845360825,
      "grad_norm": 1.659878134727478,
      "learning_rate": 1.5465892483311608e-05,
      "loss": 0.013,
      "step": 20300
    },
    {
      "epoch": 7.234269463206541,
      "grad_norm": 9.393564224243164,
      "learning_rate": 1.5367144606390962e-05,
      "loss": 0.0197,
      "step": 20350
    },
    {
      "epoch": 7.252044081052257,
      "grad_norm": 0.3154943287372589,
      "learning_rate": 1.5268396729470317e-05,
      "loss": 0.0236,
      "step": 20400
    },
    {
      "epoch": 7.269818698897974,
      "grad_norm": 0.9715698957443237,
      "learning_rate": 1.516964885254967e-05,
      "loss": 0.022,
      "step": 20450
    },
    {
      "epoch": 7.28759331674369,
      "grad_norm": 6.734008312225342,
      "learning_rate": 1.5070900975629023e-05,
      "loss": 0.0235,
      "step": 20500
    },
    {
      "epoch": 7.305367934589406,
      "grad_norm": 0.48950523138046265,
      "learning_rate": 1.4972153098708378e-05,
      "loss": 0.0162,
      "step": 20550
    },
    {
      "epoch": 7.323142552435122,
      "grad_norm": 0.1208631843328476,
      "learning_rate": 1.4873405221787731e-05,
      "loss": 0.0081,
      "step": 20600
    },
    {
      "epoch": 7.340917170280839,
      "grad_norm": 0.33859094977378845,
      "learning_rate": 1.4774657344867084e-05,
      "loss": 0.0296,
      "step": 20650
    },
    {
      "epoch": 7.3586917881265554,
      "grad_norm": 1.3676371574401855,
      "learning_rate": 1.4675909467946439e-05,
      "loss": 0.0349,
      "step": 20700
    },
    {
      "epoch": 7.3764664059722715,
      "grad_norm": 0.09577565640211105,
      "learning_rate": 1.4577161591025792e-05,
      "loss": 0.0138,
      "step": 20750
    },
    {
      "epoch": 7.394241023817988,
      "grad_norm": 0.14141781628131866,
      "learning_rate": 1.4478413714105147e-05,
      "loss": 0.0184,
      "step": 20800
    },
    {
      "epoch": 7.412015641663705,
      "grad_norm": 17.97411346435547,
      "learning_rate": 1.43796658371845e-05,
      "loss": 0.0119,
      "step": 20850
    },
    {
      "epoch": 7.429790259509421,
      "grad_norm": 0.5679872035980225,
      "learning_rate": 1.4280917960263853e-05,
      "loss": 0.0269,
      "step": 20900
    },
    {
      "epoch": 7.447564877355137,
      "grad_norm": 7.091108798980713,
      "learning_rate": 1.4182170083343208e-05,
      "loss": 0.0231,
      "step": 20950
    },
    {
      "epoch": 7.465339495200853,
      "grad_norm": 7.476688385009766,
      "learning_rate": 1.4083422206422561e-05,
      "loss": 0.0243,
      "step": 21000
    },
    {
      "epoch": 7.48311411304657,
      "grad_norm": 16.096927642822266,
      "learning_rate": 1.3984674329501916e-05,
      "loss": 0.0307,
      "step": 21050
    },
    {
      "epoch": 7.500888730892286,
      "grad_norm": 0.12378940731287003,
      "learning_rate": 1.3885926452581269e-05,
      "loss": 0.014,
      "step": 21100
    },
    {
      "epoch": 7.518663348738002,
      "grad_norm": 1.1370888948440552,
      "learning_rate": 1.3787178575660625e-05,
      "loss": 0.015,
      "step": 21150
    },
    {
      "epoch": 7.536437966583718,
      "grad_norm": 0.01781442202627659,
      "learning_rate": 1.3688430698739978e-05,
      "loss": 0.0204,
      "step": 21200
    },
    {
      "epoch": 7.554212584429434,
      "grad_norm": 0.39498719573020935,
      "learning_rate": 1.3589682821819333e-05,
      "loss": 0.0193,
      "step": 21250
    },
    {
      "epoch": 7.571987202275151,
      "grad_norm": 10.802887916564941,
      "learning_rate": 1.3490934944898686e-05,
      "loss": 0.0214,
      "step": 21300
    },
    {
      "epoch": 7.589761820120867,
      "grad_norm": 0.23430699110031128,
      "learning_rate": 1.3392187067978041e-05,
      "loss": 0.0215,
      "step": 21350
    },
    {
      "epoch": 7.6075364379665835,
      "grad_norm": 0.5930611491203308,
      "learning_rate": 1.3293439191057394e-05,
      "loss": 0.0085,
      "step": 21400
    },
    {
      "epoch": 7.6253110558123005,
      "grad_norm": 4.631789684295654,
      "learning_rate": 1.3194691314136747e-05,
      "loss": 0.0062,
      "step": 21450
    },
    {
      "epoch": 7.643085673658017,
      "grad_norm": 3.6533143520355225,
      "learning_rate": 1.3095943437216102e-05,
      "loss": 0.014,
      "step": 21500
    },
    {
      "epoch": 7.660860291503733,
      "grad_norm": 5.585588455200195,
      "learning_rate": 1.2997195560295455e-05,
      "loss": 0.0174,
      "step": 21550
    },
    {
      "epoch": 7.678634909349449,
      "grad_norm": 0.24457326531410217,
      "learning_rate": 1.289844768337481e-05,
      "loss": 0.0097,
      "step": 21600
    },
    {
      "epoch": 7.696409527195165,
      "grad_norm": 0.399944931268692,
      "learning_rate": 1.2799699806454163e-05,
      "loss": 0.0267,
      "step": 21650
    },
    {
      "epoch": 7.714184145040882,
      "grad_norm": 4.587031364440918,
      "learning_rate": 1.2700951929533516e-05,
      "loss": 0.0157,
      "step": 21700
    },
    {
      "epoch": 7.731958762886598,
      "grad_norm": 1.0641957521438599,
      "learning_rate": 1.260220405261287e-05,
      "loss": 0.0314,
      "step": 21750
    },
    {
      "epoch": 7.749733380732314,
      "grad_norm": 0.00043630439904518425,
      "learning_rate": 1.2503456175692224e-05,
      "loss": 0.013,
      "step": 21800
    },
    {
      "epoch": 7.76750799857803,
      "grad_norm": 0.307888925075531,
      "learning_rate": 1.2404708298771577e-05,
      "loss": 0.0104,
      "step": 21850
    },
    {
      "epoch": 7.785282616423747,
      "grad_norm": 0.004741224460303783,
      "learning_rate": 1.230596042185093e-05,
      "loss": 0.0106,
      "step": 21900
    },
    {
      "epoch": 7.803057234269463,
      "grad_norm": 0.2272728681564331,
      "learning_rate": 1.2207212544930285e-05,
      "loss": 0.0323,
      "step": 21950
    },
    {
      "epoch": 7.820831852115179,
      "grad_norm": 0.012067064642906189,
      "learning_rate": 1.2108464668009638e-05,
      "loss": 0.0169,
      "step": 22000
    },
    {
      "epoch": 7.8386064699608955,
      "grad_norm": 16.232454299926758,
      "learning_rate": 1.2009716791088991e-05,
      "loss": 0.008,
      "step": 22050
    },
    {
      "epoch": 7.8563810878066125,
      "grad_norm": 0.014901572838425636,
      "learning_rate": 1.1910968914168346e-05,
      "loss": 0.0131,
      "step": 22100
    },
    {
      "epoch": 7.874155705652329,
      "grad_norm": 0.4652104675769806,
      "learning_rate": 1.1812221037247699e-05,
      "loss": 0.0149,
      "step": 22150
    },
    {
      "epoch": 7.891930323498045,
      "grad_norm": 0.6025275588035583,
      "learning_rate": 1.1713473160327054e-05,
      "loss": 0.0149,
      "step": 22200
    },
    {
      "epoch": 7.909704941343761,
      "grad_norm": 0.10012450069189072,
      "learning_rate": 1.1614725283406408e-05,
      "loss": 0.0284,
      "step": 22250
    },
    {
      "epoch": 7.927479559189478,
      "grad_norm": 0.35829704999923706,
      "learning_rate": 1.1515977406485761e-05,
      "loss": 0.0304,
      "step": 22300
    },
    {
      "epoch": 7.945254177035194,
      "grad_norm": 0.012082740664482117,
      "learning_rate": 1.1417229529565116e-05,
      "loss": 0.017,
      "step": 22350
    },
    {
      "epoch": 7.96302879488091,
      "grad_norm": 0.6443495750427246,
      "learning_rate": 1.131848165264447e-05,
      "loss": 0.0308,
      "step": 22400
    },
    {
      "epoch": 7.980803412726626,
      "grad_norm": 0.6383218765258789,
      "learning_rate": 1.1219733775723822e-05,
      "loss": 0.0186,
      "step": 22450
    },
    {
      "epoch": 7.998578030572343,
      "grad_norm": 0.257968008518219,
      "learning_rate": 1.1120985898803177e-05,
      "loss": 0.0107,
      "step": 22500
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.1208906173706055,
      "eval_runtime": 604.2024,
      "eval_samples_per_second": 8.275,
      "eval_steps_per_second": 0.518,
      "step": 22504
    },
    {
      "epoch": 8.016352648418058,
      "grad_norm": 0.003806808963418007,
      "learning_rate": 1.102223802188253e-05,
      "loss": 0.004,
      "step": 22550
    },
    {
      "epoch": 8.034127266263775,
      "grad_norm": 1.0774216651916504,
      "learning_rate": 1.0923490144961885e-05,
      "loss": 0.0039,
      "step": 22600
    },
    {
      "epoch": 8.051901884109492,
      "grad_norm": 0.23761799931526184,
      "learning_rate": 1.0824742268041238e-05,
      "loss": 0.0009,
      "step": 22650
    },
    {
      "epoch": 8.069676501955207,
      "grad_norm": 0.046118687838315964,
      "learning_rate": 1.0725994391120591e-05,
      "loss": 0.0005,
      "step": 22700
    },
    {
      "epoch": 8.087451119800924,
      "grad_norm": 0.0029594632796943188,
      "learning_rate": 1.0627246514199946e-05,
      "loss": 0.0027,
      "step": 22750
    },
    {
      "epoch": 8.105225737646641,
      "grad_norm": 0.28368955850601196,
      "learning_rate": 1.0528498637279299e-05,
      "loss": 0.0007,
      "step": 22800
    },
    {
      "epoch": 8.123000355492357,
      "grad_norm": 0.14156892895698547,
      "learning_rate": 1.0429750760358654e-05,
      "loss": 0.0024,
      "step": 22850
    },
    {
      "epoch": 8.140774973338074,
      "grad_norm": 0.0043943170458078384,
      "learning_rate": 1.0331002883438007e-05,
      "loss": 0.0067,
      "step": 22900
    },
    {
      "epoch": 8.158549591183789,
      "grad_norm": 0.1044846847653389,
      "learning_rate": 1.023225500651736e-05,
      "loss": 0.0019,
      "step": 22950
    },
    {
      "epoch": 8.176324209029506,
      "grad_norm": 0.019554242491722107,
      "learning_rate": 1.0133507129596715e-05,
      "loss": 0.007,
      "step": 23000
    },
    {
      "epoch": 8.194098826875223,
      "grad_norm": 0.2638789415359497,
      "learning_rate": 1.0034759252676068e-05,
      "loss": 0.0102,
      "step": 23050
    },
    {
      "epoch": 8.211873444720938,
      "grad_norm": 0.25855761766433716,
      "learning_rate": 9.936011375755423e-06,
      "loss": 0.002,
      "step": 23100
    },
    {
      "epoch": 8.229648062566655,
      "grad_norm": 0.02359800599515438,
      "learning_rate": 9.837263498834776e-06,
      "loss": 0.0024,
      "step": 23150
    },
    {
      "epoch": 8.24742268041237,
      "grad_norm": 0.06583662331104279,
      "learning_rate": 9.738515621914129e-06,
      "loss": 0.0024,
      "step": 23200
    },
    {
      "epoch": 8.265197298258087,
      "grad_norm": 0.022833874449133873,
      "learning_rate": 9.639767744993484e-06,
      "loss": 0.0056,
      "step": 23250
    },
    {
      "epoch": 8.282971916103804,
      "grad_norm": 0.3533087968826294,
      "learning_rate": 9.541019868072837e-06,
      "loss": 0.0007,
      "step": 23300
    },
    {
      "epoch": 8.30074653394952,
      "grad_norm": 0.2428024262189865,
      "learning_rate": 9.442271991152191e-06,
      "loss": 0.0019,
      "step": 23350
    },
    {
      "epoch": 8.318521151795236,
      "grad_norm": 0.009600929915904999,
      "learning_rate": 9.343524114231545e-06,
      "loss": 0.0067,
      "step": 23400
    },
    {
      "epoch": 8.336295769640953,
      "grad_norm": 0.03677733615040779,
      "learning_rate": 9.244776237310898e-06,
      "loss": 0.001,
      "step": 23450
    },
    {
      "epoch": 8.354070387486669,
      "grad_norm": 0.3349238932132721,
      "learning_rate": 9.146028360390252e-06,
      "loss": 0.0091,
      "step": 23500
    },
    {
      "epoch": 8.371845005332386,
      "grad_norm": 0.05760077014565468,
      "learning_rate": 9.047280483469605e-06,
      "loss": 0.0007,
      "step": 23550
    },
    {
      "epoch": 8.3896196231781,
      "grad_norm": 0.0912783071398735,
      "learning_rate": 8.94853260654896e-06,
      "loss": 0.0061,
      "step": 23600
    },
    {
      "epoch": 8.407394241023818,
      "grad_norm": 0.03064713068306446,
      "learning_rate": 8.849784729628313e-06,
      "loss": 0.001,
      "step": 23650
    },
    {
      "epoch": 8.425168858869535,
      "grad_norm": 34.20170211791992,
      "learning_rate": 8.751036852707666e-06,
      "loss": 0.0059,
      "step": 23700
    },
    {
      "epoch": 8.44294347671525,
      "grad_norm": 8.356893539428711,
      "learning_rate": 8.652288975787021e-06,
      "loss": 0.0027,
      "step": 23750
    },
    {
      "epoch": 8.460718094560967,
      "grad_norm": 0.03729575127363205,
      "learning_rate": 8.553541098866374e-06,
      "loss": 0.0022,
      "step": 23800
    },
    {
      "epoch": 8.478492712406684,
      "grad_norm": 0.164215549826622,
      "learning_rate": 8.454793221945729e-06,
      "loss": 0.0018,
      "step": 23850
    },
    {
      "epoch": 8.4962673302524,
      "grad_norm": 0.020495377480983734,
      "learning_rate": 8.356045345025082e-06,
      "loss": 0.0059,
      "step": 23900
    },
    {
      "epoch": 8.514041948098116,
      "grad_norm": 1.230102300643921,
      "learning_rate": 8.257297468104435e-06,
      "loss": 0.0022,
      "step": 23950
    },
    {
      "epoch": 8.531816565943831,
      "grad_norm": 0.004049832001328468,
      "learning_rate": 8.15854959118379e-06,
      "loss": 0.0083,
      "step": 24000
    },
    {
      "epoch": 8.549591183789548,
      "grad_norm": 0.026036929339170456,
      "learning_rate": 8.059801714263143e-06,
      "loss": 0.0038,
      "step": 24050
    },
    {
      "epoch": 8.567365801635265,
      "grad_norm": 0.024204066023230553,
      "learning_rate": 7.961053837342498e-06,
      "loss": 0.0082,
      "step": 24100
    },
    {
      "epoch": 8.58514041948098,
      "grad_norm": 0.015157505869865417,
      "learning_rate": 7.862305960421851e-06,
      "loss": 0.0032,
      "step": 24150
    },
    {
      "epoch": 8.602915037326698,
      "grad_norm": 0.21495230495929718,
      "learning_rate": 7.763558083501204e-06,
      "loss": 0.0023,
      "step": 24200
    },
    {
      "epoch": 8.620689655172415,
      "grad_norm": 0.016495496034622192,
      "learning_rate": 7.664810206580559e-06,
      "loss": 0.0004,
      "step": 24250
    },
    {
      "epoch": 8.63846427301813,
      "grad_norm": 5.280154705047607,
      "learning_rate": 7.566062329659912e-06,
      "loss": 0.0014,
      "step": 24300
    },
    {
      "epoch": 8.656238890863847,
      "grad_norm": 0.1333584040403366,
      "learning_rate": 7.467314452739266e-06,
      "loss": 0.0047,
      "step": 24350
    },
    {
      "epoch": 8.674013508709562,
      "grad_norm": 0.05659382417798042,
      "learning_rate": 7.3685665758186205e-06,
      "loss": 0.0094,
      "step": 24400
    },
    {
      "epoch": 8.691788126555279,
      "grad_norm": 0.0807715505361557,
      "learning_rate": 7.2698186988979745e-06,
      "loss": 0.0145,
      "step": 24450
    },
    {
      "epoch": 8.709562744400996,
      "grad_norm": 0.0062654754146933556,
      "learning_rate": 7.171070821977328e-06,
      "loss": 0.0005,
      "step": 24500
    },
    {
      "epoch": 8.727337362246711,
      "grad_norm": 5.382486820220947,
      "learning_rate": 7.072322945056682e-06,
      "loss": 0.0092,
      "step": 24550
    },
    {
      "epoch": 8.745111980092428,
      "grad_norm": 0.1279572695493698,
      "learning_rate": 6.973575068136036e-06,
      "loss": 0.0039,
      "step": 24600
    },
    {
      "epoch": 8.762886597938145,
      "grad_norm": 0.0018649041885510087,
      "learning_rate": 6.874827191215389e-06,
      "loss": 0.0011,
      "step": 24650
    },
    {
      "epoch": 8.78066121578386,
      "grad_norm": 0.04390382394194603,
      "learning_rate": 6.776079314294743e-06,
      "loss": 0.0029,
      "step": 24700
    },
    {
      "epoch": 8.798435833629577,
      "grad_norm": 0.01796046644449234,
      "learning_rate": 6.677331437374097e-06,
      "loss": 0.0129,
      "step": 24750
    },
    {
      "epoch": 8.816210451475293,
      "grad_norm": 9.940909385681152,
      "learning_rate": 6.578583560453451e-06,
      "loss": 0.005,
      "step": 24800
    },
    {
      "epoch": 8.83398506932101,
      "grad_norm": 0.029939960688352585,
      "learning_rate": 6.479835683532805e-06,
      "loss": 0.0022,
      "step": 24850
    },
    {
      "epoch": 8.851759687166727,
      "grad_norm": 0.023974284529685974,
      "learning_rate": 6.381087806612158e-06,
      "loss": 0.0011,
      "step": 24900
    },
    {
      "epoch": 8.869534305012442,
      "grad_norm": 0.11251004785299301,
      "learning_rate": 6.282339929691512e-06,
      "loss": 0.0045,
      "step": 24950
    },
    {
      "epoch": 8.887308922858159,
      "grad_norm": 0.03471915423870087,
      "learning_rate": 6.183592052770866e-06,
      "loss": 0.004,
      "step": 25000
    },
    {
      "epoch": 8.905083540703874,
      "grad_norm": 0.09389368444681168,
      "learning_rate": 6.08484417585022e-06,
      "loss": 0.0036,
      "step": 25050
    },
    {
      "epoch": 8.922858158549591,
      "grad_norm": 0.6369958519935608,
      "learning_rate": 5.986096298929574e-06,
      "loss": 0.0026,
      "step": 25100
    },
    {
      "epoch": 8.940632776395308,
      "grad_norm": 0.16351714730262756,
      "learning_rate": 5.887348422008927e-06,
      "loss": 0.0018,
      "step": 25150
    },
    {
      "epoch": 8.958407394241023,
      "grad_norm": 5.141849994659424,
      "learning_rate": 5.788600545088281e-06,
      "loss": 0.0019,
      "step": 25200
    },
    {
      "epoch": 8.97618201208674,
      "grad_norm": 0.003091721562668681,
      "learning_rate": 5.689852668167635e-06,
      "loss": 0.0005,
      "step": 25250
    },
    {
      "epoch": 8.993956629932457,
      "grad_norm": 0.05735329911112785,
      "learning_rate": 5.591104791246989e-06,
      "loss": 0.0031,
      "step": 25300
    },
    {
      "epoch": 9.0,
      "eval_loss": 2.1984546184539795,
      "eval_runtime": 577.1052,
      "eval_samples_per_second": 8.664,
      "eval_steps_per_second": 0.542,
      "step": 25317
    },
    {
      "epoch": 9.011731247778172,
      "grad_norm": 0.038485899567604065,
      "learning_rate": 5.492356914326343e-06,
      "loss": 0.0005,
      "step": 25350
    },
    {
      "epoch": 9.02950586562389,
      "grad_norm": 0.0017954271752387285,
      "learning_rate": 5.393609037405696e-06,
      "loss": 0.0005,
      "step": 25400
    },
    {
      "epoch": 9.047280483469605,
      "grad_norm": 0.01315393764525652,
      "learning_rate": 5.29486116048505e-06,
      "loss": 0.0001,
      "step": 25450
    },
    {
      "epoch": 9.065055101315322,
      "grad_norm": 0.017244892194867134,
      "learning_rate": 5.196113283564404e-06,
      "loss": 0.0001,
      "step": 25500
    },
    {
      "epoch": 9.082829719161039,
      "grad_norm": 0.03264477103948593,
      "learning_rate": 5.0973654066437575e-06,
      "loss": 0.0002,
      "step": 25550
    },
    {
      "epoch": 9.100604337006754,
      "grad_norm": 8.238811492919922,
      "learning_rate": 4.9986175297231114e-06,
      "loss": 0.0004,
      "step": 25600
    },
    {
      "epoch": 9.11837895485247,
      "grad_norm": 0.03837086260318756,
      "learning_rate": 4.8998696528024645e-06,
      "loss": 0.0001,
      "step": 25650
    },
    {
      "epoch": 9.136153572698188,
      "grad_norm": 0.00121244415640831,
      "learning_rate": 4.8011217758818184e-06,
      "loss": 0.0001,
      "step": 25700
    },
    {
      "epoch": 9.153928190543903,
      "grad_norm": 0.006553085520863533,
      "learning_rate": 4.702373898961172e-06,
      "loss": 0.0001,
      "step": 25750
    },
    {
      "epoch": 9.17170280838962,
      "grad_norm": 0.0266557764261961,
      "learning_rate": 4.603626022040526e-06,
      "loss": 0.0001,
      "step": 25800
    },
    {
      "epoch": 9.189477426235335,
      "grad_norm": 0.008120319806039333,
      "learning_rate": 4.50487814511988e-06,
      "loss": 0.0002,
      "step": 25850
    },
    {
      "epoch": 9.207252044081052,
      "grad_norm": 0.07375027984380722,
      "learning_rate": 4.406130268199233e-06,
      "loss": 0.0003,
      "step": 25900
    },
    {
      "epoch": 9.22502666192677,
      "grad_norm": 0.09778734296560287,
      "learning_rate": 4.307382391278587e-06,
      "loss": 0.0005,
      "step": 25950
    },
    {
      "epoch": 9.242801279772484,
      "grad_norm": 0.0010339489672333002,
      "learning_rate": 4.208634514357942e-06,
      "loss": 0.0001,
      "step": 26000
    },
    {
      "epoch": 9.260575897618201,
      "grad_norm": 0.003347486024722457,
      "learning_rate": 4.109886637437296e-06,
      "loss": 0.0011,
      "step": 26050
    },
    {
      "epoch": 9.278350515463918,
      "grad_norm": 0.4461658000946045,
      "learning_rate": 4.011138760516649e-06,
      "loss": 0.0001,
      "step": 26100
    },
    {
      "epoch": 9.296125133309634,
      "grad_norm": 0.012580038979649544,
      "learning_rate": 3.912390883596003e-06,
      "loss": 0.0013,
      "step": 26150
    },
    {
      "epoch": 9.31389975115535,
      "grad_norm": 0.029961690306663513,
      "learning_rate": 3.813643006675357e-06,
      "loss": 0.0001,
      "step": 26200
    },
    {
      "epoch": 9.331674369001066,
      "grad_norm": 0.012346300296485424,
      "learning_rate": 3.714895129754711e-06,
      "loss": 0.0001,
      "step": 26250
    },
    {
      "epoch": 9.349448986846783,
      "grad_norm": 0.007982033304870129,
      "learning_rate": 3.6161472528340643e-06,
      "loss": 0.0004,
      "step": 26300
    },
    {
      "epoch": 9.3672236046925,
      "grad_norm": 0.007043380755931139,
      "learning_rate": 3.5173993759134182e-06,
      "loss": 0.0001,
      "step": 26350
    },
    {
      "epoch": 9.384998222538215,
      "grad_norm": 0.023817503824830055,
      "learning_rate": 3.4186514989927718e-06,
      "loss": 0.0001,
      "step": 26400
    },
    {
      "epoch": 9.402772840383932,
      "grad_norm": 0.002639390528202057,
      "learning_rate": 3.3199036220721257e-06,
      "loss": 0.0002,
      "step": 26450
    },
    {
      "epoch": 9.420547458229649,
      "grad_norm": 0.0067183636128902435,
      "learning_rate": 3.2211557451514796e-06,
      "loss": 0.0001,
      "step": 26500
    },
    {
      "epoch": 9.438322076075364,
      "grad_norm": 0.010346699506044388,
      "learning_rate": 3.122407868230833e-06,
      "loss": 0.0054,
      "step": 26550
    },
    {
      "epoch": 9.456096693921081,
      "grad_norm": 0.061633795499801636,
      "learning_rate": 3.023659991310187e-06,
      "loss": 0.0001,
      "step": 26600
    },
    {
      "epoch": 9.473871311766796,
      "grad_norm": 0.10509724915027618,
      "learning_rate": 2.9249121143895405e-06,
      "loss": 0.0002,
      "step": 26650
    },
    {
      "epoch": 9.491645929612513,
      "grad_norm": 0.003980192821472883,
      "learning_rate": 2.8261642374688945e-06,
      "loss": 0.0001,
      "step": 26700
    },
    {
      "epoch": 9.50942054745823,
      "grad_norm": 0.002331604016944766,
      "learning_rate": 2.7274163605482484e-06,
      "loss": 0.0001,
      "step": 26750
    },
    {
      "epoch": 9.527195165303946,
      "grad_norm": 0.00790785439312458,
      "learning_rate": 2.6286684836276023e-06,
      "loss": 0.0001,
      "step": 26800
    },
    {
      "epoch": 9.544969783149662,
      "grad_norm": 0.03206417337059975,
      "learning_rate": 2.5299206067069563e-06,
      "loss": 0.0001,
      "step": 26850
    },
    {
      "epoch": 9.562744400995378,
      "grad_norm": 0.0371410995721817,
      "learning_rate": 2.4311727297863098e-06,
      "loss": 0.0001,
      "step": 26900
    },
    {
      "epoch": 9.580519018841095,
      "grad_norm": 0.06058989092707634,
      "learning_rate": 2.3324248528656637e-06,
      "loss": 0.0007,
      "step": 26950
    },
    {
      "epoch": 9.598293636686812,
      "grad_norm": 0.0005078763933852315,
      "learning_rate": 2.233676975945017e-06,
      "loss": 0.0003,
      "step": 27000
    },
    {
      "epoch": 9.616068254532527,
      "grad_norm": 0.04360901564359665,
      "learning_rate": 2.134929099024371e-06,
      "loss": 0.0001,
      "step": 27050
    },
    {
      "epoch": 9.633842872378244,
      "grad_norm": 0.010247024707496166,
      "learning_rate": 2.036181222103725e-06,
      "loss": 0.0001,
      "step": 27100
    },
    {
      "epoch": 9.65161749022396,
      "grad_norm": 0.021429162472486496,
      "learning_rate": 1.9374333451830786e-06,
      "loss": 0.0001,
      "step": 27150
    },
    {
      "epoch": 9.669392108069676,
      "grad_norm": 0.010152870789170265,
      "learning_rate": 1.8386854682624323e-06,
      "loss": 0.0002,
      "step": 27200
    },
    {
      "epoch": 9.687166725915393,
      "grad_norm": 0.005502792540937662,
      "learning_rate": 1.7399375913417862e-06,
      "loss": 0.0002,
      "step": 27250
    },
    {
      "epoch": 9.70494134376111,
      "grad_norm": 0.0034921809565275908,
      "learning_rate": 1.64118971442114e-06,
      "loss": 0.0006,
      "step": 27300
    },
    {
      "epoch": 9.722715961606825,
      "grad_norm": 0.042474567890167236,
      "learning_rate": 1.5424418375004939e-06,
      "loss": 0.0001,
      "step": 27350
    },
    {
      "epoch": 9.740490579452542,
      "grad_norm": 0.013957022689282894,
      "learning_rate": 1.4436939605798476e-06,
      "loss": 0.0001,
      "step": 27400
    },
    {
      "epoch": 9.758265197298257,
      "grad_norm": 0.0025827456265687943,
      "learning_rate": 1.3449460836592013e-06,
      "loss": 0.0002,
      "step": 27450
    },
    {
      "epoch": 9.776039815143974,
      "grad_norm": 0.002780440030619502,
      "learning_rate": 1.2461982067385552e-06,
      "loss": 0.0001,
      "step": 27500
    },
    {
      "epoch": 9.793814432989691,
      "grad_norm": 0.002033338649198413,
      "learning_rate": 1.147450329817909e-06,
      "loss": 0.0001,
      "step": 27550
    },
    {
      "epoch": 9.811589050835407,
      "grad_norm": 0.0051705422811210155,
      "learning_rate": 1.0487024528972629e-06,
      "loss": 0.0001,
      "step": 27600
    },
    {
      "epoch": 9.829363668681124,
      "grad_norm": 0.0038483852986246347,
      "learning_rate": 9.499545759766165e-07,
      "loss": 0.0001,
      "step": 27650
    },
    {
      "epoch": 9.847138286526839,
      "grad_norm": 0.02871999889612198,
      "learning_rate": 8.512066990559703e-07,
      "loss": 0.0001,
      "step": 27700
    },
    {
      "epoch": 9.864912904372556,
      "grad_norm": 0.007970530539751053,
      "learning_rate": 7.524588221353241e-07,
      "loss": 0.0001,
      "step": 27750
    },
    {
      "epoch": 9.882687522218273,
      "grad_norm": 0.01472611352801323,
      "learning_rate": 6.537109452146778e-07,
      "loss": 0.0001,
      "step": 27800
    },
    {
      "epoch": 9.900462140063988,
      "grad_norm": 0.10739794373512268,
      "learning_rate": 5.549630682940318e-07,
      "loss": 0.0001,
      "step": 27850
    },
    {
      "epoch": 9.918236757909705,
      "grad_norm": 0.004336676560342312,
      "learning_rate": 4.562151913733855e-07,
      "loss": 0.0001,
      "step": 27900
    },
    {
      "epoch": 9.936011375755422,
      "grad_norm": 0.005765958223491907,
      "learning_rate": 3.574673144527393e-07,
      "loss": 0.0001,
      "step": 27950
    },
    {
      "epoch": 9.953785993601137,
      "grad_norm": 0.013477746397256851,
      "learning_rate": 2.5871943753209307e-07,
      "loss": 0.0001,
      "step": 28000
    },
    {
      "epoch": 9.971560611446854,
      "grad_norm": 0.015398344956338406,
      "learning_rate": 1.5997156061144687e-07,
      "loss": 0.0001,
      "step": 28050
    },
    {
      "epoch": 9.98933522929257,
      "grad_norm": 0.0036036833189427853,
      "learning_rate": 6.122368369080065e-08,
      "loss": 0.0001,
      "step": 28100
    },
    {
      "epoch": 10.0,
      "eval_loss": 2.1501524448394775,
      "eval_runtime": 586.7461,
      "eval_samples_per_second": 8.522,
      "eval_steps_per_second": 0.533,
      "step": 28130
    }
  ],
  "logging_steps": 50,
  "max_steps": 28130,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.77255879496541e+19,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
